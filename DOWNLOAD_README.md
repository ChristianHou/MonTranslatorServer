# NLLB-MoE 54B æ¨¡å‹ä¸‹è½½æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä¸‹è½½facebook/nllb-moe-54bæ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•ã€‚

## ğŸ“‹ æ¨¡å‹ä¿¡æ¯

- **æ¨¡å‹åç§°**: facebook/nllb-moe-54b
- **æ¨¡å‹å¤§å°**: ~200GB
- **æ¨¡å‹ç±»å‹**: Mixture of Experts (MoE)
- **æ”¯æŒè¯­è¨€**: 200+ç§è¯­è¨€
- **ä¸‹è½½åœ°å€**: https://huggingface.co/facebook/nllb-moe-54b

## âš ï¸ é‡è¦æé†’

1. **å·¨å¤§æ–‡ä»¶**: è¿™ä¸ªæ¨¡å‹éå¸¸å¤§ï¼ˆçº¦200GBï¼‰ï¼Œä¸‹è½½éœ€è¦å¾ˆé•¿æ—¶é—´
2. **ç£ç›˜ç©ºé—´**: ç¡®ä¿è‡³å°‘æœ‰250GBå¯ç”¨ç£ç›˜ç©ºé—´
3. **ç½‘ç»œè¦æ±‚**: éœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥ï¼Œä¸‹è½½è¿‡ç¨‹å¯èƒ½æŒç»­å‡ å¤©
4. **ä¸­æ–­æ¢å¤**: æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¯ä»¥éšæ—¶ä¸­æ–­å¹¶ç»§ç»­

## ğŸš€ ä¸‹è½½æ–¹æ³•

### æ–¹æ³•1ï¼šWindowsæ‰¹å¤„ç†è„šæœ¬ï¼ˆæ¨èï¼‰

```cmd
# åŒå‡»è¿è¡Œæˆ–åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œ
download_nllb_moe.bat
```

**è„šæœ¬ä¼šè‡ªåŠ¨**ï¼š
- æ£€æŸ¥Pythonç¯å¢ƒ
- å®‰è£…å¿…è¦çš„ä¾èµ–
- æ£€æŸ¥ç£ç›˜ç©ºé—´
- ä¸‹è½½æ¨¡å‹åˆ°`./cache`ç›®å½•

### æ–¹æ³•2ï¼šPythonè„šæœ¬

```bash
# å®‰è£…ä¾èµ–
pip install huggingface_hub

# è¿è¡Œä¸‹è½½è„šæœ¬
python download_nllb_moe_simple.py
```

### æ–¹æ³•3ï¼šæ‰‹åŠ¨ä¸‹è½½

```python
from huggingface_hub import snapshot_download

# ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
model_path = snapshot_download(
    repo_id="facebook/nllb-moe-54b",
    local_dir="./cache/models--facebook--nllb-moe-54b",
    local_dir_use_symlinks=False,
    resume_download=True
)
```

## ğŸ“ æ–‡ä»¶ç»“æ„

ä¸‹è½½å®Œæˆåï¼Œæ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š

```
MonTranslatorServer/
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ models--facebook--nllb-moe-54b/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”œâ”€â”€ pytorch_model.bin.index.json
â”‚       â”œâ”€â”€ pytorch_model-00001-of-00008.bin
â”‚       â”œâ”€â”€ pytorch_model-00002-of-00008.bin
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ pytorch_model-00008-of-00008.bin
â”œâ”€â”€ download_nllb_moe_simple.py
â”œâ”€â”€ download_nllb_moe.bat
â””â”€â”€ ...
```

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜1ï¼šç½‘ç»œè¿æ¥é—®é¢˜

```bash
# è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
export HTTP_PROXY=http://your-proxy:8080
export HTTPS_PROXY=http://your-proxy:8080

# æˆ–è€…ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

### é—®é¢˜2ï¼šç£ç›˜ç©ºé—´ä¸è¶³

```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ¸…ç†ç©ºé—´
rm -rf ~/.cache/huggingface/*
```

### é—®é¢˜3ï¼šä¸‹è½½ä¸­æ–­

è„šæœ¬æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°è¿è¡Œå³å¯ç»§ç»­ï¼š

```bash
python download_nllb_moe_simple.py
```

### é—®é¢˜4ï¼šä¾èµ–å®‰è£…å¤±è´¥

```bash
# æ‰‹åŠ¨å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install huggingface_hub

# ä½¿ç”¨å›½å†…æº
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple huggingface_hub
```

## ğŸ“Š ä¸‹è½½è¿›åº¦ç›‘æ§

### æŸ¥çœ‹ä¸‹è½½è¿›åº¦

```bash
# æŸ¥çœ‹ä¸‹è½½è¿›ç¨‹
ps aux | grep huggingface

# æŸ¥çœ‹ç½‘ç»œé€Ÿåº¦
nload

# æŸ¥çœ‹ç£ç›˜ä½¿ç”¨
du -sh ./cache
```

### ç›‘æ§æ—¥å¿—

```bash
# æŸ¥çœ‹ä¸‹è½½æ—¥å¿—
tail -f download_nllb_moe.log
```

## ğŸ¯ ä½¿ç”¨æ¨¡å‹

ä¸‹è½½å®Œæˆåï¼Œå¯ä»¥åœ¨MonTranslatorä¸­ä½¿ç”¨ï¼š

1. **æ›´æ–°é…ç½®**ï¼š
   ```ini
   # config/config.ini
   [SETTINGS]
   seq_translate_model = facebook/nllb-moe-54b
   ```

2. **è¿è¡Œå®¹å™¨**ï¼š
   ```bash
   docker run --gpus all -p 8000:8000 \
     -v $(pwd)/cache:/app/cache \
     -v $(pwd)/files:/app/files \
     -v $(pwd)/config:/app/config \
     mon-translator:gpu
   ```

## ğŸ”„ å…¶ä»–ä¸‹è½½é€‰é¡¹

### å¦‚æœåªéœ€è¦è¾ƒå°çš„æ¨¡å‹

è€ƒè™‘ä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ¨¡å‹ï¼š
- `facebook/nllb-200-distilled-600M` (~2.5GB)
- `facebook/nllb-200-1.3B` (~5GB)
- `facebook/nllb-200-3.3B` (~12GB)

### åˆ†æ‰¹ä¸‹è½½

å¦‚æœç½‘ç»œä¸ç¨³å®šï¼Œå¯ä»¥åˆ†æ‰¹ä¸‹è½½ï¼š

```python
from huggingface_hub import HfApi, snapshot_download

# åªä¸‹è½½é…ç½®æ–‡ä»¶
snapshot_download(
    repo_id="facebook/nllb-moe-54b",
    allow_patterns=["*.json", "*.txt"],
    local_dir="./cache/models--facebook--nllb-moe-54b"
)

# ç„¶åä¸‹è½½æ¨¡å‹æ–‡ä»¶
snapshot_download(
    repo_id="facebook/nllb-moe-54b",
    allow_patterns=["*.bin"],
    local_dir="./cache/models--facebook--nllb-moe-54b"
)
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å¤§å° | å†…å­˜éœ€æ±‚ | ç¿»è¯‘é€Ÿåº¦ | è´¨é‡ |
|------|------|----------|----------|------|
| NLLB-600M | 2.5GB | 4GB | å¿« | è‰¯å¥½ |
| NLLB-1.3B | 5GB | 8GB | ä¸­ç­‰ | ä¼˜ç§€ |
| NLLB-3.3B | 12GB | 16GB | ä¸­ç­‰ | ä¼˜ç§€ |
| **NLLB-MoE-54B** | **200GB** | **32GB+** | **æ…¢** | **æœ€ä½³** |

## ğŸ‰ å®Œæˆä¸‹è½½å

1. **éªŒè¯ä¸‹è½½**ï¼š
   ```bash
   ls -la ./cache/models--facebook--nllb-moe-54b/
   ```

2. **æµ‹è¯•æ¨¡å‹**ï¼š
   ```python
   from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

   model_path = "./cache/models--facebook--nllb-moe-54b"
   tokenizer = AutoTokenizer.from_pretrained(model_path)
   model = AutoModelForSeq2SeqLM.from_pretrained(model_path)

   print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
   ```

3. **å¼€å§‹ä½¿ç”¨**ï¼š
   - å¯åŠ¨MonTranslatorå®¹å™¨
   - äº«å—é«˜è´¨é‡çš„ç¿»è¯‘æœåŠ¡

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœä¸‹è½½è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
3. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ï¼š`download_nllb_moe.log`
4. é‡è¯•ä¸‹è½½ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰

---

**æ³¨æ„**: è¿™ä¸ªæ¨¡å‹éå¸¸å¤§ï¼Œè¯·ç¡®ä¿æ‚¨ç¡®å®éœ€è¦è¿™ä¹ˆé«˜è´¨é‡çš„ç¿»è¯‘æœåŠ¡ã€‚å¦‚æœåªæ˜¯æµ‹è¯•ç”¨é€”ï¼Œå»ºè®®å…ˆä½¿ç”¨è¾ƒå°çš„æ¨¡å‹ã€‚


