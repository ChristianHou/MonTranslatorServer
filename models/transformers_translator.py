#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Transformersç¿»è¯‘å™¨å®ç°
ä½œä¸ºctranslate2çš„æ›¿ä»£æ–¹æ¡ˆï¼Œæä¾›æ›´ç®€å•çš„APIå’Œæ›´å¥½çš„å…¼å®¹æ€§
"""

import logging
import threading
import time
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class TaskType(Enum):
    """ä»»åŠ¡ç±»å‹æšä¸¾"""
    TEXT = "text"
    FILE = "file"


@dataclass
class TranslationResult:
    """ç¿»è¯‘ç»“æœæ•°æ®ç±»"""
    source_text: str
    translated_text: str
    source_lang: str
    target_lang: str
    processing_time: float
    task_type: TaskType
    success: bool
    error_message: Optional[str] = None


class TransformersTranslator:
    """ä½¿ç”¨Transformersçš„ç¿»è¯‘å™¨"""
    
    def __init__(self, model_name: str, device: str = "cpu"):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨
        
        Args:
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            device: è®¾å¤‡ç±»å‹ ("cpu", "cuda", "cuda:0" ç­‰)
        """
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self._lock = threading.Lock()
        self._initialized = False
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_requests = 0
        self.successful_requests = 0
        self.total_processing_time = 0.0
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–Transformersç¿»è¯‘å™¨: {model_name} on {device}")
    
    def _ensure_initialized(self):
        """ç¡®ä¿æ¨¡å‹å·²åˆå§‹åŒ–"""
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        try:
            logger.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {self.model_name}")
            
            from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
            import torch
            
            # åŠ è½½tokenizer
            start_time = time.time()
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            tokenizer_time = time.time() - start_time
            logger.info(f"âœ… TokenizeråŠ è½½å®Œæˆï¼Œè€—æ—¶: {tokenizer_time:.2f}ç§’")
            
            # åŠ è½½æ¨¡å‹
            start_time = time.time()
            self.model = AutoModelForSeq2SeqLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device != "cpu" else torch.float32,
                low_cpu_mem_usage=True
            )
            
            # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            if self.device != "cpu":
                self.model = self.model.to(self.device)
            
            model_time = time.time() - start_time
            logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {model_time:.2f}ç§’")
            
            self._initialized = True
            logger.info(f"ğŸ‰ Transformersç¿»è¯‘å™¨åˆå§‹åŒ–å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ {self.model_name}: {e}")
    
    def translate_text(self, text: str, source_lang: str, target_lang: str, 
                      task_type: TaskType = TaskType.TEXT) -> TranslationResult:
        """
        ç¿»è¯‘å•ä¸ªæ–‡æœ¬
        
        Args:
            text: æºæ–‡æœ¬
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            task_type: ä»»åŠ¡ç±»å‹
            
        Returns:
            TranslationResult: ç¿»è¯‘ç»“æœ
        """
        start_time = time.time()
        self.total_requests += 1
        
        try:
            self._ensure_initialized()
            
            # æ„å»ºç›®æ ‡è¯­è¨€å‰ç¼€
            target_prefix = self._get_target_prefix(target_lang)
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                padding=True, 
                truncation=True, 
                max_length=512
            )
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            if self.device != "cpu":
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆç¿»è¯‘
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=512,
                    num_beams=5,
                    early_stopping=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=False
                )
            
            # è§£ç è¾“å‡º
            translated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ç§»é™¤ç›®æ ‡è¯­è¨€å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if target_prefix and translated_text.startswith(target_prefix):
                translated_text = translated_text[len(target_prefix):].strip()
            
            processing_time = time.time() - start_time
            self.successful_requests += 1
            self.total_processing_time += processing_time
            
            logger.debug(f"âœ… ç¿»è¯‘å®Œæˆ: '{text[:50]}...' -> '{translated_text[:50]}...' ({processing_time:.3f}s)")
            
            return TranslationResult(
                source_text=text,
                translated_text=translated_text,
                source_lang=source_lang,
                target_lang=target_lang,
                processing_time=processing_time,
                task_type=task_type,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_msg = f"ç¿»è¯‘å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            return TranslationResult(
                source_text=text,
                translated_text="",
                source_lang=source_lang,
                target_lang=target_lang,
                processing_time=processing_time,
                task_type=task_type,
                success=False,
                error_message=error_msg
            )
    
    def translate_batch(self, texts: List[str], source_lang: str, target_lang: str,
                       task_type: TaskType = TaskType.TEXT, batch_size: int = 8) -> List[TranslationResult]:
        """
        æ‰¹é‡ç¿»è¯‘æ–‡æœ¬
        
        Args:
            texts: æºæ–‡æœ¬åˆ—è¡¨
            source_lang: æºè¯­è¨€ä»£ç 
            target_lang: ç›®æ ‡è¯­è¨€ä»£ç 
            task_type: ä»»åŠ¡ç±»å‹
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            List[TranslationResult]: ç¿»è¯‘ç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        self.total_requests += len(texts)
        
        try:
            self._ensure_initialized()
            
            results = []
            target_prefix = self._get_target_prefix(target_lang)
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                logger.debug(f"ğŸ§ª å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch_texts)} ä¸ªæ–‡æœ¬")
                
                # ç¼–ç æ‰¹æ¬¡
                inputs = self.tokenizer(
                    batch_texts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )
                
                # ç§»åŠ¨åˆ°è®¾å¤‡
                if self.device != "cpu":
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ç”Ÿæˆç¿»è¯‘
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=512,
                        num_beams=5,
                        early_stopping=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                        do_sample=False
                    )
                
                # è§£ç è¾“å‡º
                for j, output in enumerate(outputs):
                    translated_text = self.tokenizer.decode(output, skip_special_tokens=True)
                    
                    # ç§»é™¤ç›®æ ‡è¯­è¨€å‰ç¼€
                    if target_prefix and translated_text.startswith(target_prefix):
                        translated_text = translated_text[len(target_prefix):].strip()
                    
                    # åˆ›å»ºç»“æœ
                    result = TranslationResult(
                        source_text=batch_texts[j],
                        translated_text=translated_text,
                        source_lang=source_lang,
                        target_lang=target_lang,
                        processing_time=0.0,  # å•ä¸ªæ–‡æœ¬çš„å¤„ç†æ—¶é—´æ— æ³•å‡†ç¡®è®¡ç®—
                        task_type=task_type,
                        success=True
                    )
                    results.append(result)
            
            batch_time = time.time() - start_time
            self.successful_requests += len(texts)
            self.total_processing_time += batch_time
            
            logger.info(f"âœ… æ‰¹é‡ç¿»è¯‘å®Œæˆ: {len(texts)} ä¸ªæ–‡æœ¬ï¼Œè€—æ—¶: {batch_time:.3f}ç§’")
            return results
            
        except Exception as e:
            batch_time = time.time() - start_time
            error_msg = f"æ‰¹é‡ç¿»è¯‘å¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            
            # è¿”å›é”™è¯¯ç»“æœ
            error_results = []
            for text in texts:
                result = TranslationResult(
                    source_text=text,
                    translated_text="",
                    source_lang=source_lang,
                    target_lang=target_lang,
                    processing_time=batch_time / len(texts),
                    task_type=task_type,
                    success=False,
                    error_message=error_msg
                )
                error_results.append(result)
            
            return error_results
    
    def _get_target_prefix(self, target_lang: str) -> str:
        """è·å–ç›®æ ‡è¯­è¨€å‰ç¼€"""
        # NLLBæ¨¡å‹çš„è¯­è¨€å‰ç¼€æ˜ å°„
        lang_prefixes = {
            "cmn_Hans": "cmn_Hans",
            "cmn_Hant": "cmn_Hant", 
            "eng_Latn": "eng_Latn",
            "mon_Cyrl": "mon_Cyrl",
            "mon_Mong": "mon_Mong"
        }
        return lang_prefixes.get(target_lang, "")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        avg_time = (self.total_processing_time / self.successful_requests 
                   if self.successful_requests > 0 else 0.0)
        
        return {
            "total_requests": self.total_requests,
            "successful_requests": self.successful_requests,
            "failed_requests": self.total_requests - self.successful_requests,
            "success_rate": (self.successful_requests / self.total_requests 
                           if self.total_requests > 0 else 0.0),
            "total_processing_time": self.total_processing_time,
            "average_processing_time": avg_time,
            "model_name": self.model_name,
            "device": self.device,
            "initialized": self._initialized
        }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            
            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            
            self._initialized = False
            logger.info("ğŸ§¹ Transformersç¿»è¯‘å™¨èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.warning(f"âš ï¸  æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")


class TransformersTranslatorPool:
    """Transformersç¿»è¯‘å™¨æ± """
    
    def __init__(self, model_name: str, pool_size: int = 4, device: str = "cpu"):
        """
        åˆå§‹åŒ–ç¿»è¯‘å™¨æ± 
        
        Args:
            model_name: æ¨¡å‹åç§°
            pool_size: æ± å¤§å°
            device: è®¾å¤‡ç±»å‹
        """
        self.model_name = model_name
        self.pool_size = pool_size
        self.device = device
        self.translators = []
        self._lock = threading.Lock()
        self._initialized = False
        
        logger.info(f"ğŸŠ åˆå§‹åŒ–Transformersç¿»è¯‘å™¨æ± : {pool_size} ä¸ªå®ä¾‹")
    
    def initialize(self):
        """åˆå§‹åŒ–ç¿»è¯‘å™¨æ± """
        if not self._initialized:
            with self._lock:
                if not self._initialized:
                    logger.info(f"ğŸš€ åˆ›å»º {self.pool_size} ä¸ªç¿»è¯‘å™¨å®ä¾‹...")
                    
                    for i in range(self.pool_size):
                        translator = TransformersTranslator(
                            model_name=self.model_name,
                            device=self.device
                        )
                        self.translators.append(translator)
                    
                    self._initialized = True
                    logger.info(f"âœ… ç¿»è¯‘å™¨æ± åˆå§‹åŒ–å®Œæˆ")
    
    def get_translator(self) -> TransformersTranslator:
        """è·å–ä¸€ä¸ªå¯ç”¨çš„ç¿»è¯‘å™¨"""
        self.initialize()
        
        with self._lock:
            if not self.translators:
                raise RuntimeError("ç¿»è¯‘å™¨æ± ä¸ºç©º")
            
            # ç®€å•çš„è½®è¯¢åˆ†é…
            translator = self.translators.pop(0)
            self.translators.append(translator)
            return translator
    
    def translate_text(self, text: str, source_lang: str, target_lang: str,
                      task_type: TaskType = TaskType.TEXT) -> TranslationResult:
        """ç¿»è¯‘å•ä¸ªæ–‡æœ¬"""
        translator = self.get_translator()
        return translator.translate_text(text, source_lang, target_lang, task_type)
    
    def translate_batch(self, texts: List[str], source_lang: str, target_lang: str,
                       task_type: TaskType = TaskType.TEXT, batch_size: int = 8) -> List[TranslationResult]:
        """æ‰¹é‡ç¿»è¯‘æ–‡æœ¬"""
        translator = self.get_translator()
        return translator.translate_batch(texts, source_lang, target_lang, task_type, batch_size)
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """è·å–æ± ç»Ÿè®¡ä¿¡æ¯"""
        if not self.translators:
            return {"status": "æœªåˆå§‹åŒ–", "pool_size": self.pool_size}
        
        # æ±‡æ€»æ‰€æœ‰ç¿»è¯‘å™¨çš„ç»Ÿè®¡ä¿¡æ¯
        total_stats = {
            "pool_size": self.pool_size,
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_processing_time": 0.0,
            "average_processing_time": 0.0
        }
        
        for translator in self.translators:
            stats = translator.get_stats()
            total_stats["total_requests"] += stats["total_requests"]
            total_stats["successful_requests"] += stats["successful_requests"]
            total_stats["failed_requests"] += stats["failed_requests"]
            total_stats["total_processing_time"] += stats["total_processing_time"]
        
        if total_stats["successful_requests"] > 0:
            total_stats["average_processing_time"] = (
                total_stats["total_processing_time"] / total_stats["successful_requests"]
            )
        
        total_stats["success_rate"] = (
            total_stats["successful_requests"] / total_stats["total_requests"]
            if total_stats["total_requests"] > 0 else 0.0
        )
        
        return total_stats
    
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰ç¿»è¯‘å™¨"""
        with self._lock:
            for translator in self.translators:
                translator.cleanup()
            self.translators.clear()
            self._initialized = False
            logger.info("ğŸ§¹ ç¿»è¯‘å™¨æ± æ¸…ç†å®Œæˆ")


# å…¨å±€ç¿»è¯‘å™¨æ± å®ä¾‹
_translator_pool: Optional[TransformersTranslatorPool] = None


def get_translator_pool(model_name: str, pool_size: int = 4, device: str = "cpu") -> TransformersTranslatorPool:
    """è·å–å…¨å±€ç¿»è¯‘å™¨æ± å®ä¾‹"""
    global _translator_pool
    
    if _translator_pool is None:
        _translator_pool = TransformersTranslatorPool(model_name, pool_size, device)
    
    return _translator_pool


def cleanup_translator_pool():
    """æ¸…ç†å…¨å±€ç¿»è¯‘å™¨æ± """
    global _translator_pool
    
    if _translator_pool is not None:
        _translator_pool.cleanup()
        _translator_pool = None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºç¿»è¯‘å™¨æ± 
    pool = get_translator_pool("facebook/nllb-200-distilled-600M", pool_size=2)
    
    # æµ‹è¯•ç¿»è¯‘
    test_texts = [
        "Hello, how are you?",
        "The weather is beautiful today.",
        "Machine learning is fascinating."
    ]
    
    try:
        # å•ä¸ªç¿»è¯‘æµ‹è¯•
        result = pool.translate_text("Hello world!", "eng_Latn", "cmn_Hans")
        print(f"å•ä¸ªç¿»è¯‘: {result}")
        
        # æ‰¹é‡ç¿»è¯‘æµ‹è¯•
        results = pool.translate_batch(test_texts, "eng_Latn", "cmn_Hans")
        print(f"æ‰¹é‡ç¿»è¯‘: {len(results)} ä¸ªç»“æœ")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = pool.get_pool_stats()
        print(f"æ± ç»Ÿè®¡: {stats}")
        
    finally:
        cleanup_translator_pool()
