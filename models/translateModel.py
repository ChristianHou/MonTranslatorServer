import ctranslate2
import transformers
import pandas as pd
import threading
from openpyxl import load_workbook
from utils.config_manager import config_manager
import logging
import time
import os
from fastapi import HTTPException
from docx import Document
from tqdm import tqdm
import sys


class TranslatorSingleton:
    """ç¿»è¯‘å™¨å•ä¾‹ç±»ï¼Œæ”¯æŒå¤šGPUè´Ÿè½½å‡è¡¡å’Œä»»åŠ¡ç±»å‹æ„ŸçŸ¥çš„å®ä¾‹åˆ†é…"""
    
    _instance = None
    _initialized = False
    _cpu_instances = []
    _cuda_instances = []
    _gpu_devices = []
    _gpu_memory_info = {}
    _text_translation_instances = []
    _file_translation_instances = []
    _task_type_distribution = {"text": 0.6, "file": 0.4}
    _lock = threading.Lock()  # æ·»åŠ ç¼ºå¤±çš„é”
    _tokenizers = {}  # æ·»åŠ ç¼ºå¤±çš„tokenizerså­—å…¸
    _instance_locks = []  # æ·»åŠ ç¼ºå¤±çš„instance_locksåˆ—è¡¨
    
    @classmethod
    def _check_cuda_support(cls):
        """æ£€æŸ¥CUDAæ”¯æŒ - åªä½¿ç”¨PyTorchæ£€æŸ¥"""
        try:
            import torch
            
            # æ£€æŸ¥PyTorch CUDAæ”¯æŒ
            if not torch.cuda.is_available():
                logging.info("â„¹ï¸  PyTorch CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                return False
            
            gpu_count = torch.cuda.device_count()
            if gpu_count == 0:
                logging.info("â„¹ï¸  PyTorchæœªæ£€æµ‹åˆ°GPUè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
                return False
            
            logging.info(f"âœ… PyTorchæ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            test_model_path = config_manager.get_model_path(config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL"))
            if not os.path.exists(test_model_path):
                logging.warning(f"âš ï¸  æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {test_model_path}")
                return False
            
            # å°è¯•åˆ›å»ºCUDAè®¾å¤‡ä¸Šçš„translatoræ¥æµ‹è¯•
            try:
                test_translator = ctranslate2.Translator(
                    test_model_path,
                    device="cuda",  # ctranslate2ä½¿ç”¨"cuda"è€Œä¸æ˜¯"cuda:0"
                    inter_threads=1,
                    intra_threads=1,
                    compute_type="float32"  # T4ä½¿ç”¨float32
                )
                
                # å¦‚æœæˆåŠŸåˆ›å»ºï¼Œè¯´æ˜CUDAæ”¯æŒæ­£å¸¸
                del test_translator
                logging.info("âœ… ctranslate2 CUDAè®¾å¤‡æµ‹è¯•æˆåŠŸï¼ŒGPUæ¨¡å¼å¯ç”¨")
                return True
                
            except Exception as e:
                logging.warning(f"âš ï¸  ctranslate2 CUDAè®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
                logging.info("â„¹ï¸  è™½ç„¶PyTorchæ£€æµ‹åˆ°CUDAï¼Œä½†ctranslate2æ— æ³•ä½¿ç”¨GPUï¼Œå°†å›é€€åˆ°CPUæ¨¡å¼")
                return False
                
        except ImportError:
            logging.warning("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æŸ¥CUDAæ”¯æŒ")
            return False
        except Exception as e:
            logging.warning(f"âš ï¸  CUDAæ”¯æŒæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    @classmethod
    def _safe_cuda_call(cls, func, *args, default=None, **kwargs):
        """å®‰å…¨çš„CUDAè°ƒç”¨ï¼Œä½¿ç”¨PyTorchè€Œä¸æ˜¯ctranslate2"""
        try:
            import torch
            
            if not torch.cuda.is_available():
                return default
            
            # å¯¹äºGPUç›¸å…³çš„è°ƒç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨PyTorch
            if func.__name__ == 'get_device_count':
                return torch.cuda.device_count()
            elif func.__name__ == 'get_device_memory_info':
                gpu_id = args[0] if args else 0
                if gpu_id < torch.cuda.device_count():
                    props = torch.cuda.get_device_properties(gpu_id)
                    # åˆ›å»ºä¸€ä¸ªç±»ä¼¼ctranslate2çš„å†…å­˜ä¿¡æ¯å¯¹è±¡
                    class MockMemoryInfo:
                        def __init__(self, total, free):
                            self.total = total
                            self.free = free
                    
                    # è·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
                    torch.cuda.set_device(gpu_id)
                    allocated = torch.cuda.memory_allocated(gpu_id)
                    reserved = torch.cuda.memory_reserved(gpu_id)
                    free = props.total_memory - allocated
                    
                    return MockMemoryInfo(props.total_memory, free)
                else:
                    return default
            else:
                # å…¶ä»–CUDAè°ƒç”¨ï¼Œè¿”å›é»˜è®¤å€¼
                return default
                
        except Exception as e:
            logging.warning(f"CUDAè°ƒç”¨å¤±è´¥ {func.__name__}: {e}")
            return default

    @classmethod
    def initialize_models(cls, num_cpu_models=2, num_gpu_models=4):
        """åˆå§‹åŒ–æ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¤šGPUéƒ¨ç½²ï¼Œè‡ªåŠ¨å›é€€åˆ°CPUæ¨¡å¼"""
        # é¿å…é•¿æ—¶é—´æŒæœ‰ç±»é”
        with cls._lock:
            if cls._initialized:
                return
        
        # åœ¨é”å¤–è¿›è¡Œæ¨¡å‹åˆå§‹åŒ–
        cpu_instances = []
        cuda_instances = []
        instance_locks = []
        
        # è·å–å¯ç”¨çš„GPUæ•°é‡å’Œè®¾å¤‡ä¿¡æ¯
        gpu_count = 0
        cuda_support = cls._check_cuda_support()
        
        if cuda_support:
            try:
                # ä½¿ç”¨PyTorchè·å–GPUä¿¡æ¯
                import torch
                gpu_count = torch.cuda.device_count()
                cls._gpu_devices = list(range(gpu_count))
                logging.info(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUè®¾å¤‡: {cls._gpu_devices}")
                
                # åˆå§‹åŒ–GPUå†…å­˜ä¿¡æ¯
                for gpu_id in cls._gpu_devices:
                    try:
                        props = torch.cuda.get_device_properties(gpu_id)
                        total_memory = props.total_memory
                        allocated = torch.cuda.memory_allocated(gpu_id)
                        free_memory = total_memory - allocated
                        
                        cls._gpu_memory_info[gpu_id] = {
                            'total': total_memory,
                            'free': free_memory,
                            'used': allocated
                        }
                        logging.info(f"GPU {gpu_id}: {props.name}, æ€»å†…å­˜ {total_memory/1024**3:.2f}GB, å·²ç”¨ {allocated/1024**3:.2f}GB")
                    except Exception as e:
                        logging.warning(f"æ— æ³•è·å–GPU {gpu_id} å†…å­˜ä¿¡æ¯: {e}")
                        cls._gpu_memory_info[gpu_id] = {'total': 0, 'free': 0, 'used': 0}
            except Exception as e:
                logging.warning(f"âš ï¸  CUDAæ”¯æŒæ£€æŸ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼: {e}")
                gpu_count = 0
        else:
            logging.info("â„¹ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            gpu_count = 0
        
        # åˆ›å»ºCPUå®ä¾‹
        for i in range(num_cpu_models):
            instance_lock = threading.Lock()
            try:
                translator = ctranslate2.Translator(
                    config_manager.get_model_path(config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL")),
                    inter_threads=4,
                    intra_threads=1
                )
                
                cpu_instances.append({
                    "translator": translator,
                    "task_count": threading.Semaphore(10),
                    "lock": instance_lock,
                    "id": f"cpu_{i}",
                    "type": "cpu",
                    "response_times": [],
                    "last_used": time.time()
                })
                instance_locks.append(instance_lock)
                logging.info(f"âœ… æˆåŠŸåˆ›å»ºCPUå®ä¾‹: cpu_{i}")
                
            except Exception as e:
                logging.error(f"åˆ›å»ºCPUå®ä¾‹å¤±è´¥: {e}")
                continue
        
        # åˆ›å»ºGPUå®ä¾‹ï¼Œæ”¯æŒå¤šGPUè´Ÿè½½å‡è¡¡
        if gpu_count > 0 and cuda_support:
            try:
                # è®¡ç®—æ¯ä¸ªGPUä¸Šåº”è¯¥åˆ›å»ºçš„å®ä¾‹æ•°é‡ï¼Œç¡®ä¿å‡åŒ€åˆ†å¸ƒ
                instances_per_gpu = num_gpu_models // gpu_count
                remaining_instances = num_gpu_models % gpu_count
                
                # è®¡ç®—æ–‡æœ¬ç¿»è¯‘å’Œæ–‡ä»¶ç¿»è¯‘çš„å®ä¾‹æ•°é‡
                total_gpu_instances = num_gpu_models
                text_instances = int(total_gpu_instances * cls._task_type_distribution["text"])
                file_instances = total_gpu_instances - text_instances
                
                text_instance_count = 0
                file_instance_count = 0
                
                for gpu_id in cls._gpu_devices:
                    # ä¸ºæ¯ä¸ªGPUåˆ†é…å®ä¾‹æ•°é‡
                    current_gpu_instances = instances_per_gpu
                    if remaining_instances > 0:
                        current_gpu_instances += 1
                        remaining_instances -= 1
                    
                    for i in range(current_gpu_instances):
                        instance_lock = threading.Lock()
                        try:
                            # ä¸ºT4æ˜¾å¡ä¼˜åŒ–å‚æ•° - ä½¿ç”¨æ­£ç¡®çš„ctranslate2 deviceå‚æ•°
                            translator = ctranslate2.Translator(
                                config_manager.get_model_path(config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL")),
                                device="cuda",  # ctranslate2ä½¿ç”¨"cuda"è€Œä¸æ˜¯"cuda:0"
                                inter_threads=2,
                                intra_threads=1,
                                device_index=gpu_id,
                                compute_type="float32"  # T4ä½¿ç”¨float32ï¼Œfloat16å¯èƒ½ä¸è¢«æ”¯æŒ
                            )
                            
                            # æ ¹æ®å®ä¾‹æ•°é‡åˆ†é…ä»»åŠ¡ç±»å‹
                            if text_instance_count < text_instances:
                                task_type = "text"
                                text_instance_count += 1
                                cls._text_translation_instances.append(f"cuda_{gpu_id}_{i}")
                            else:
                                task_type = "file"
                                file_instance_count += 1
                                cls._file_translation_instances.append(f"cuda_{gpu_id}_{i}")
                            
                            instance_info = {
                                "translator": translator,
                                "task_count": threading.Semaphore(config_manager.getint('GPU', 'GPU_CONCURRENT_LIMIT', 4)),
                                "lock": instance_lock,
                                "id": f"cuda_{gpu_id}_{i}",
                                "type": "cuda",
                                "gpu_id": gpu_id,
                                "task_type": task_type,  # æ·»åŠ ä»»åŠ¡ç±»å‹æ ‡ç­¾
                                "response_times": [],
                                "last_used": time.time(),
                                "memory_usage": 0,
                                "total_tasks": 0,
                                "successful_tasks": 0,
                                "text_tasks": 0,      # æ–‡æœ¬ç¿»è¯‘ä»»åŠ¡è®¡æ•°
                                "file_tasks": 0       # æ–‡ä»¶ç¿»è¯‘ä»»åŠ¡è®¡æ•°
                            }
                            
                            cuda_instances.append(instance_info)
                            instance_locks.append(instance_lock)
                            
                            logging.info(f"âœ… æˆåŠŸåˆ›å»ºGPUå®ä¾‹: cuda_{gpu_id}_{i} åœ¨GPU {gpu_id}ä¸Šï¼Œä»»åŠ¡ç±»å‹: {task_type}")
                            
                        except Exception as e:
                            logging.error(f"åœ¨GPU {gpu_id}ä¸Šåˆ›å»ºå®ä¾‹å¤±è´¥: {e}")
                            continue
            except Exception as e:
                logging.error(f"åˆ›å»ºGPUå®ä¾‹æ—¶å‡ºé”™: {e}")
                logging.info("âš ï¸  å°†å›é€€åˆ°çº¯CPUæ¨¡å¼")
        else:
            logging.info("â„¹ï¸  ä½¿ç”¨çº¯CPUæ¨¡å¼ï¼Œå°†å¢åŠ CPUå®ä¾‹æ•°é‡")
            # å¦‚æœGPUä¸å¯ç”¨ï¼Œå¢åŠ CPUå®ä¾‹æ•°é‡
            additional_cpu_instances = min(4, num_gpu_models)  # æœ€å¤šå¢åŠ 4ä¸ªCPUå®ä¾‹
            for i in range(additional_cpu_instances):
                instance_lock = threading.Lock()
                try:
                    translator = ctranslate2.Translator(
                        config_manager.get_model_path(config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL")),
                        inter_threads=4,
                        intra_threads=1
                    )
                    
                    cpu_instances.append({
                        "translator": translator,
                        "task_count": threading.Semaphore(10),
                        "lock": instance_lock,
                        "id": f"cpu_extra_{i}",
                        "type": "cpu",
                        "response_times": [],
                        "last_used": time.time()
                    })
                    instance_locks.append(instance_lock)
                    logging.info(f"âœ… æˆåŠŸåˆ›å»ºé¢å¤–CPUå®ä¾‹: cpu_extra_{i}")
                    
                except Exception as e:
                    logging.error(f"åˆ›å»ºé¢å¤–CPUå®ä¾‹å¤±è´¥: {e}")
                    continue
        
        # å†æ¬¡è·å–é”å¹¶è®¾ç½®åˆå§‹åŒ–çŠ¶æ€
        with cls._lock:
            if not cls._initialized:  # Double-check
                cls._cpu_instances = cpu_instances
                cls._cuda_instances = cuda_instances
                cls._instance_locks = instance_locks
                cls._initialized = True
                
                if cuda_instances:
                    logging.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚CPUå®ä¾‹: {len(cpu_instances)}, GPUå®ä¾‹: {len(cuda_instances)}")
                else:
                    logging.info(f"âœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼ˆCPUæ¨¡å¼ï¼‰ã€‚CPUå®ä¾‹: {len(cpu_instances)}")
                    logging.info("â„¹ï¸  ç³»ç»Ÿå°†åœ¨CPUæ¨¡å¼ä¸‹è¿è¡Œï¼Œæ€§èƒ½å¯èƒ½è¾ƒæ…¢ä½†åŠŸèƒ½å®Œæ•´")
    
    @classmethod
    def cleanup_resources(cls):
        """æ¸…ç†èµ„æº"""
        with cls._lock:
            if not cls._initialized:
                return
            
            # æ¸…ç†CPUå®ä¾‹
            for instance in cls._cpu_instances:
                try:
                    if "translator" in instance:
                        del instance["translator"]
                except Exception as e:
                    logging.warning(f"æ¸…ç†CPUå®ä¾‹æ—¶å‡ºé”™: {e}")
            
            # æ¸…ç†GPUå®ä¾‹
            for instance in cls._cuda_instances:
                try:
                    if "translator" in instance:
                        del instance["translator"]
                except Exception as e:
                    logging.warning(f"æ¸…ç†GPUå®ä¾‹æ—¶å‡ºé”™: {e}")
            
            # æ¸…ç†tokenizerç¼“å­˜
            for tokenizer in cls._tokenizers.values():
                try:
                    del tokenizer
                except Exception as e:
                    logging.warning(f"æ¸…ç†tokenizeræ—¶å‡ºé”™: {e}")
            
            cls._cpu_instances.clear()
            cls._cuda_instances.clear()
            cls._tokenizers.clear()
            cls._instance_locks.clear()
            cls._initialized = False
            cls._gpu_devices.clear()
            cls._gpu_memory_info.clear()
            logging.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")

    @classmethod
    def _load_tokenizer(cls, src_lang: str):
        """åŠ è½½tokenizerï¼Œæ·»åŠ å†…å­˜ç®¡ç†"""
        with cls._lock:
            if (src_lang, "tokenizer") not in cls._tokenizers:
                try:
                    # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœç¼“å­˜è¿‡å¤šåˆ™æ¸…ç†
                    if len(cls._tokenizers) > 10:  # é™åˆ¶ç¼“å­˜æ•°é‡
                        # æ¸…ç†æœ€æ—§çš„tokenizer
                        oldest_key = min(cls._tokenizers.keys(), key=lambda k: cls._tokenizers[k].get('last_used', 0))
                        if oldest_key in cls._tokenizers:
                            del cls._tokenizers[oldest_key]
                            logging.info(f"æ¸…ç†æ—§tokenizerç¼“å­˜: {oldest_key}")
                    
                    # è·å–æ¨¡å‹åç§°
                    model_name = config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL")
                    
                    # å°è¯•ä»TOKENIZER_LISTè·å–tokenizerè·¯å¾„
                    tokenizer_path = None
                    if config_manager.config.has_section('TOKENIZER_LIST'):
                        tokenizer_path = config_manager.config.get('TOKENIZER_LIST', model_name, fallback=None)
                    
                    if tokenizer_path and os.path.exists(tokenizer_path):
                        # ä½¿ç”¨é…ç½®çš„tokenizerè·¯å¾„
                        logging.info(f"ä½¿ç”¨é…ç½®çš„tokenizerè·¯å¾„: {tokenizer_path}")
                        tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)
                    else:
                        # å›é€€åˆ°æ¨¡å‹è·¯å¾„
                        model_path = config_manager.get_model_path(model_name)
                        logging.info(f"ä½¿ç”¨æ¨¡å‹è·¯å¾„åŠ è½½tokenizer: {model_path}")
                        tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)
                    
                    cls._tokenizers[(src_lang, "tokenizer")] = {
                        'tokenizer': tokenizer,
                        'last_used': time.time()
                    }
                    logging.info(f"åŠ è½½tokenizeræˆåŠŸ: {src_lang}")
                except Exception as e:
                    logging.error(f"åŠ è½½tokenizerå¤±è´¥: {e}")
                    raise
            
            # æ›´æ–°ä½¿ç”¨æ—¶é—´
            cls._tokenizers[(src_lang, "tokenizer")]['last_used'] = time.time()
            return cls._tokenizers[(src_lang, "tokenizer")]['tokenizer']

    @classmethod
    def _get_least_loaded_model(cls, use_cuda=False, task_type="text"):
        """è·å–è´Ÿè½½æœ€è½»çš„æ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒä»»åŠ¡ç±»å‹æ„ŸçŸ¥çš„è´Ÿè½½å‡è¡¡ç­–ç•¥"""
        start_time = time.time()
        instances = cls._cuda_instances if use_cuda else cls._cpu_instances
        
        # æ€§èƒ½ç›‘æ§ï¼šè®°å½•å®ä¾‹é€‰æ‹©å¼€å§‹
        logging.info(f"ğŸš€ [è´Ÿè½½å‡è¡¡] å¼€å§‹é€‰æ‹©æ¨¡å‹å®ä¾‹ - ä»»åŠ¡ç±»å‹: {task_type}, ä½¿ç”¨CUDA: {use_cuda}")
        logging.info(f"ğŸ“Š [è´Ÿè½½å‡è¡¡] å¯ç”¨å®ä¾‹æ•°é‡: {len(instances)}")
        
        if not instances:
            raise RuntimeError("No model instances available. Call initialize_models() first.")
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç­›é€‰å®ä¾‹
        if use_cuda and task_type:
            # ä¼˜å…ˆé€‰æ‹©å¯¹åº”ä»»åŠ¡ç±»å‹çš„å®ä¾‹
            preferred_instances = [inst for inst in instances if inst.get("task_type") == task_type]
            fallback_instances = [inst for inst in instances if inst.get("task_type") != task_type]
            
            # å¦‚æœé¦–é€‰å®ä¾‹å¯ç”¨ï¼Œä½¿ç”¨é¦–é€‰å®ä¾‹
            if preferred_instances:
                instances = preferred_instances
                logging.info(f"ğŸ¯ [è´Ÿè½½å‡è¡¡] ä½¿ç”¨{task_type}ä¸“ç”¨å®ä¾‹ï¼Œæ•°é‡: {len(instances)}")
            else:
                # å¦‚æœé¦–é€‰å®ä¾‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨å®ä¾‹
                instances = fallback_instances
                logging.warning(f"âš ï¸ [è´Ÿè½½å‡è¡¡] {task_type}ä¸“ç”¨å®ä¾‹ä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨å®ä¾‹ï¼Œæ•°é‡: {len(instances)}")
        
        # ä½¿ç”¨æ™ºèƒ½è´Ÿè½½å‡è¡¡ç®—æ³•
        selected_instance = None
        best_score = float('-inf')
        
        # æ›´æ–°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if use_cuda:
            cls._update_gpu_memory_usage(instances)
            logging.info(f"ğŸ”„ [GPUç›‘æ§] å·²æ›´æ–°GPUå†…å­˜ä½¿ç”¨ä¿¡æ¯")
        
        # è®°å½•æ‰€æœ‰å®ä¾‹çš„è¯¦ç»†çŠ¶æ€
        logging.info(f"ğŸ“‹ [è´Ÿè½½å‡è¡¡] å¼€å§‹è¯„ä¼°å®ä¾‹çŠ¶æ€...")
        for instance in instances:
            try:
                # ä½¿ç”¨éé˜»å¡æ–¹å¼æ£€æŸ¥ä¿¡å·é‡å¯ç”¨æ€§
                if instance["task_count"].acquire(blocking=False):
                    # ç«‹å³é‡Šæ”¾ï¼Œåªæ˜¯æ£€æŸ¥å¯ç”¨æ€§
                    instance["task_count"].release()
                    
                    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                    score = 0
                    score_details = []
                    
                    # 1. å¯ç”¨æ§½ä½è¯„åˆ†ï¼ˆæƒé‡40%ï¼‰
                    available_slots = instance["task_count"]._value
                    max_slots = config_manager.getint('GPU', 'GPU_CONCURRENT_LIMIT', 4)
                    slot_score = (available_slots / max_slots) * 40
                    score += slot_score
                    score_details.append(f"æ§½ä½: {available_slots}/{max_slots} ({slot_score:.1f})")
                    
                    # 2. ä»»åŠ¡ç±»å‹åŒ¹é…å¥–åŠ±ï¼ˆæƒé‡20%ï¼‰
                    if use_cuda and "task_type" in instance:
                        if instance["task_type"] == task_type:
                            score += 20  # ä»»åŠ¡ç±»å‹åŒ¹é…å¥–åŠ±
                            score_details.append(f"ç±»å‹åŒ¹é…: +20")
                        else:
                            score -= 10  # ä»»åŠ¡ç±»å‹ä¸åŒ¹é…æƒ©ç½š
                            score_details.append(f"ç±»å‹ä¸åŒ¹é…: -10")
                    
                    # 3. å“åº”æ—¶é—´è¯„åˆ†ï¼ˆæƒé‡25%ï¼‰
                    if instance["response_times"]:
                        avg_response_time = sum(instance["response_times"]) / len(instance["response_times"])
                        # å“åº”æ—¶é—´è¶ŠçŸ­ï¼Œè¯„åˆ†è¶Šé«˜
                        response_score = max(0, (1000 - avg_response_time) / 1000 * 25)
                        score += response_score
                        score_details.append(f"å“åº”æ—¶é—´: {avg_response_time:.1f}ms ({response_score:.1f})")
                    
                    # 4. ä»»åŠ¡æˆåŠŸç‡è¯„åˆ†ï¼ˆæƒé‡10%ï¼‰
                    if "total_tasks" in instance and instance["total_tasks"] > 0:
                        success_rate = instance["successful_tasks"] / instance["total_tasks"]
                        success_score = success_rate * 10
                        score += success_score
                        score_details.append(f"æˆåŠŸç‡: {success_rate:.2f} ({success_score:.1f})")
                    
                    # 5. GPUå†…å­˜ä½¿ç”¨è¯„åˆ†ï¼ˆæƒé‡5%ï¼‰
                    if use_cuda and "gpu_id" in instance:
                        gpu_id = instance["gpu_id"]
                        if gpu_id in cls._gpu_memory_info:
                            memory_usage_ratio = cls._gpu_memory_info[gpu_id]['used'] / cls._gpu_memory_info[gpu_id]['total']
                            # å†…å­˜ä½¿ç”¨ç‡è¶Šä½ï¼Œè¯„åˆ†è¶Šé«˜
                            memory_score = (1 - memory_usage_ratio) * 5
                            score += memory_score
                            score_details.append(f"GPUå†…å­˜: {memory_usage_ratio:.2f} ({memory_score:.1f})")
                    
                    # è®°å½•å®ä¾‹è¯¦ç»†è¯„åˆ†
                    logging.info(f"ğŸ“Š [è´Ÿè½½å‡è¡¡] å®ä¾‹ {instance['id']} è¯„åˆ†: {score:.1f} - {' | '.join(score_details)}")
                    
                    # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å®ä¾‹
                    if score > best_score:
                        best_score = score
                        selected_instance = instance
                        logging.info(f"ğŸ† [è´Ÿè½½å‡è¡¡] æ–°æœ€ä½³å®ä¾‹: {instance['id']} (è¯„åˆ†: {score:.1f})")
                        
            except Exception as e:
                logging.warning(f"âŒ [è´Ÿè½½å‡è¡¡] æ£€æŸ¥å®ä¾‹ {instance.get('id', 'unknown')} è´Ÿè½½æ—¶å‡ºé”™: {e}")
                continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯ç”¨å®ä¾‹ï¼Œä½¿ç”¨è½®è¯¢ç­–ç•¥
        if selected_instance is None:
            # éšæœºé€‰æ‹©ä¸€ä¸ªå®ä¾‹å¹¶ç­‰å¾…
            import random
            selected_instance = random.choice(instances)
            logging.warning(f"ğŸ”„ [è´Ÿè½½å‡è¡¡] ä½¿ç”¨è½®è¯¢ç­–ç•¥é€‰æ‹©å®ä¾‹: {selected_instance['id']}")
        
        # è®°å½•æœ€ç»ˆé€‰æ‹©çš„å®ä¾‹
        selection_time = (time.time() - start_time) * 1000
        logging.info(f"âœ… [è´Ÿè½½å‡è¡¡] å®ä¾‹é€‰æ‹©å®Œæˆ - é€‰æ‹©: {selected_instance['id']}, è€—æ—¶: {selection_time:.1f}ms")
        
        # å°è¯•è·å–é€‰ä¸­çš„å®ä¾‹
        try:
            # è®¾ç½®è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ— é™ç­‰å¾…
            if selected_instance["task_count"].acquire(blocking=True, timeout=30):
                selected_instance["last_used"] = time.time()
                if "total_tasks" in selected_instance:
                    selected_instance["total_tasks"] += 1
                
                # è®°å½•ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                if use_cuda and "task_type" in selected_instance:
                    if task_type == "text":
                        selected_instance["text_tasks"] += 1
                    elif task_type == "file":
                        selected_instance["file_tasks"] += 1
                
                return selected_instance
            else:
                raise RuntimeError("è·å–æ¨¡å‹å®ä¾‹è¶…æ—¶")
        except Exception as e:
            logging.error(f"è·å–æ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
            # å¦‚æœè·å–å¤±è´¥ï¼Œå°è¯•å…¶ä»–å®ä¾‹
            for instance in instances:
                if instance != selected_instance:
                    try:
                        if instance["task_count"].acquire(blocking=False):
                            instance["last_used"] = time.time()
                            if "total_tasks" in instance:
                                instance["total_tasks"] += 1
                            
                            # è®°å½•ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                            if use_cuda and "task_type" in instance:
                                if task_type == "text":
                                    instance["text_tasks"] += 1
                                elif task_type == "file":
                                    instance["file_tasks"] += 1
                            
                            return instance
                    except:
                        continue
            raise RuntimeError("æ— æ³•è·å–ä»»ä½•å¯ç”¨çš„æ¨¡å‹å®ä¾‹")

    @classmethod
    def _update_gpu_memory_usage(cls, gpu_instances):
        """æ›´æ–°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import torch
            
            for instance in gpu_instances:
                if "gpu_id" in instance:
                    gpu_id = instance["gpu_id"]
                    try:
                        # ä½¿ç”¨torchè·å–GPUå†…å­˜ä¿¡æ¯
                        if torch.cuda.is_available() and gpu_id < torch.cuda.device_count():
                            torch.cuda.set_device(gpu_id)
                            memory_info = torch.cuda.get_device_properties(gpu_id)
                            total_memory = memory_info.total_memory
                            allocated_memory = torch.cuda.memory_allocated(gpu_id)
                            cached_memory = torch.cuda.memory_reserved(gpu_id)
                            free_memory = total_memory - allocated_memory
                            
                            cls._gpu_memory_info[gpu_id] = {
                                'total': total_memory,
                                'free': free_memory,
                                'used': allocated_memory,
                                'cached': cached_memory
                            }
                            instance["memory_usage"] = allocated_memory
                        else:
                            logging.debug(f"GPU {gpu_id} ä¸å¯ç”¨æˆ–è¶…å‡ºèŒƒå›´")
                    except Exception as e:
                        logging.warning(f"æ›´æ–°GPU {gpu_id} å†…å­˜ä¿¡æ¯å¤±è´¥: {e}")
        except Exception as e:
            logging.warning(f"æ›´æ–°GPUå†…å­˜ä½¿ç”¨æƒ…å†µå¤±è´¥: {e}")

    @classmethod
    def _release_model(cls, model_instance):
        """é‡Šæ”¾æ¨¡å‹å®ä¾‹"""
        try:
            if model_instance and "task_count" in model_instance:
                model_instance["task_count"].release()
                
                # è®°å½•å“åº”æ—¶é—´
                if "last_used" in model_instance:
                    response_time = time.time() - model_instance["last_used"]
                    model_instance["response_times"].append(response_time)
                    
                    # åªä¿ç•™æœ€è¿‘çš„10ä¸ªå“åº”æ—¶é—´
                    if len(model_instance["response_times"]) > 10:
                        model_instance["response_times"] = model_instance["response_times"][-10:]
                    
                    # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                    model_instance["last_used"] = time.time()
                    
                    # è®°å½•ä»»åŠ¡æˆåŠŸ
                    if "successful_tasks" in model_instance:
                        model_instance["successful_tasks"] += 1
                    
        except Exception as e:
            logging.error(f"é‡Šæ”¾æ¨¡å‹å®ä¾‹æ—¶å‡ºé”™: {e}")

    @classmethod
    def translate_sentence(cls, text: str, src_lang: str, tgt_lang: str, use_cuda=False, via_eng=False, task_type="text") -> str:
        """ç¿»è¯‘å•ä¸ªå¥å­"""
        model_instance = None
        start_time = time.time()
        
        # æ€§èƒ½ç›‘æ§ï¼šè®°å½•ç¿»è¯‘å¼€å§‹
        logging.info(f"ğŸš€ [ç¿»è¯‘] å¼€å§‹ç¿»è¯‘å•ä¸ªå¥å­ - ä»»åŠ¡ç±»å‹: {task_type}, ä½¿ç”¨CUDA: {use_cuda}")
        logging.info(f"ğŸ“ [ç¿»è¯‘] æºè¯­è¨€: {src_lang} -> ç›®æ ‡è¯­è¨€: {tgt_lang}, æ–‡æœ¬é•¿åº¦: {len(text)}")
        
        try:
            if via_eng and src_lang != "eng_Latn" and tgt_lang != "eng_Latn":
                # é˜²æ­¢æ— é™é€’å½’ï¼šæ£€æŸ¥è¯­è¨€ç»„åˆ
                if src_lang == tgt_lang:
                    raise ValueError("æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä¸èƒ½ç›¸åŒ")
                
                logging.info(f"ğŸ”„ [ç¿»è¯‘] é€šè¿‡è‹±è¯­ä¸­è½¬ç¿»è¯‘: {src_lang} -> eng_Latn -> {tgt_lang}")
                # First translate to English
                intermediate_text = cls.translate_sentence(text, src_lang, "eng_Latn", use_cuda, False, task_type)
                # Then translate from English to target language
                return cls.translate_sentence(intermediate_text, "eng_Latn", tgt_lang, use_cuda, False, task_type)

            # è·å–æ¨¡å‹å®ä¾‹
            instance_start_time = time.time()
            model_instance = cls._get_least_loaded_model(use_cuda, task_type)
            instance_time = (time.time() - instance_start_time) * 1000
            logging.info(f"âœ… [ç¿»è¯‘] è·å–æ¨¡å‹å®ä¾‹å®Œæˆ - å®ä¾‹ID: {model_instance['id']}, è€—æ—¶: {instance_time:.1f}ms")
            
            translator = model_instance["translator"]
            tokenizer = cls._load_tokenizer(src_lang)

            # ä½¿ç”¨å®‰å…¨çš„tokenizationæ–¹æ³•
            token_start_time = time.time()
            source = cls._safe_encode_and_convert(tokenizer, text)
            token_time = (time.time() - token_start_time) * 1000
            logging.debug(f"ğŸ”¤ [ç¿»è¯‘] Tokenizationç»“æœ: {source[:10]}... (å…±{len(source)}ä¸ª), è€—æ—¶: {token_time:.1f}ms")
            
            # æ˜ å°„è¯­è¨€ä»£ç åˆ°NLLBæ ¼å¼
            mapped_tgt_lang = cls._map_language_code(tgt_lang)
            target_prefix = [[mapped_tgt_lang]]  # æ¢å¤ï¼šéœ€è¦æ˜¯List[List[str]]æ ¼å¼ï¼Œæ¯ä¸ªæ–‡æœ¬å¯¹åº”ä¸€ä¸ªåˆ—è¡¨
            
            # æ‰§è¡Œç¿»è¯‘
            inference_start_time = time.time()
            results = translator.translate_batch([source], target_prefix=[target_prefix])
            inference_time = (time.time() - inference_start_time) * 1000
            logging.info(f"âš¡ [ç¿»è¯‘] æ¨¡å‹æ¨ç†å®Œæˆ - è€—æ—¶: {inference_time:.1f}ms")
            
            # ä¿®å¤decodeè°ƒç”¨ - æ ¹æ®å®˜æ–¹demoå¤„ç†ç»“æœ
            try:
                if hasattr(results[0], 'hypotheses') and results[0].hypotheses:
                    # æ ¹æ®å®˜æ–¹demoï¼šè·³è¿‡ç¬¬ä¸€ä¸ªtoken
                    target = results[0].hypotheses[0][1:]
                    
                    # ä½¿ç”¨convert_tokens_to_ids + decode
                    if hasattr(tokenizer, 'convert_tokens_to_ids') and hasattr(tokenizer, 'decode'):
                        target_ids = tokenizer.convert_tokens_to_ids(target)
                        translated_text = tokenizer.decode(target_ids)
                    else:
                        # å›é€€ï¼šç›´æ¥ä½¿ç”¨hypotheses
                        translated_text = ' '.join(target)
                else:
                    # å›é€€å¤„ç†
                    translated_text = str(results[0])
                    
            except Exception as e:
                logging.error(f"å¤„ç†ç¿»è¯‘ç»“æœå¤±è´¥: {e}")
                # å›é€€åˆ°åŸå§‹ç»“æœ
                translated_text = str(results[0])
            
            # è®°å½•å“åº”æ—¶é—´
            response_time = time.time() - start_time
            if "response_times" in model_instance:
                model_instance["response_times"].append(response_time)
                # åªä¿ç•™æœ€è¿‘çš„10ä¸ªå“åº”æ—¶é—´
                if len(model_instance["response_times"]) > 10:
                    model_instance["response_times"] = model_instance["response_times"][-10:]
            
            return translated_text
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            logging.error(f"ç¿»è¯‘å¥å­æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=f"ç¿»è¯‘æœåŠ¡å†…éƒ¨é”™è¯¯: {e}")
        finally:
            if model_instance:
                cls._release_model(model_instance)

    @classmethod
    def translate_batch(cls, texts: list, src_lang: str, tgt_lang: str, use_cuda=False, via_eng=False, task_type="text") -> list:
        """æ‰¹é‡ç¿»è¯‘"""
        model_instance = None
        start_time = time.time()
        
        # æ€§èƒ½ç›‘æ§ï¼šè®°å½•æ‰¹é‡ç¿»è¯‘å¼€å§‹
        logging.info(f"ğŸš€ [æ‰¹é‡ç¿»è¯‘] å¼€å§‹æ‰¹é‡ç¿»è¯‘ - ä»»åŠ¡ç±»å‹: {task_type}, ä½¿ç”¨CUDA: {use_cuda}")
        logging.info(f"ğŸ“ [æ‰¹é‡ç¿»è¯‘] æºè¯­è¨€: {src_lang} -> ç›®æ ‡è¯­è¨€: {tgt_lang}, æ–‡æœ¬æ•°é‡: {len(texts)}")
        logging.info(f"ğŸ“Š [æ‰¹é‡ç¿»è¯‘] æ€»æ–‡æœ¬é•¿åº¦: {sum(len(str(t)) for t in texts)} å­—ç¬¦")
        
        try:
            if via_eng and src_lang != "eng_Latn" and tgt_lang != "eng_Latn":
                # é˜²æ­¢æ— é™é€’å½’ï¼šæ£€æŸ¥è¯­è¨€ç»„åˆ
                if src_lang == tgt_lang:
                    raise ValueError("æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€ä¸èƒ½ç›¸åŒ")
                
                logging.info(f"ğŸ”„ [æ‰¹é‡ç¿»è¯‘] é€šè¿‡è‹±è¯­ä¸­è½¬ç¿»è¯‘: {src_lang} -> eng_Latn -> {tgt_lang}")
                # First translate to English
                intermediate_texts = cls.translate_batch(texts, src_lang, "eng_Latn", use_cuda, False, task_type)
                # Then translate from English to target language
                return cls.translate_batch(intermediate_texts, "eng_Latn", tgt_lang, use_cuda, False, task_type)

            # è·å–æ¨¡å‹å®ä¾‹
            instance_start_time = time.time()
            model_instance = cls._get_least_loaded_model(use_cuda, task_type)
            instance_time = (time.time() - instance_start_time) * 1000
            logging.info(f"âœ… [æ‰¹é‡ç¿»è¯‘] è·å–æ¨¡å‹å®ä¾‹å®Œæˆ - å®ä¾‹ID: {model_instance['id']}, è€—æ—¶: {instance_time:.1f}ms")
            translator = model_instance["translator"]
            tokenizer = cls._load_tokenizer(src_lang)

            # ä½¿ç”¨å®‰å…¨çš„tokenizationæ–¹æ³•
            sources = []
            for i, text in enumerate(texts):
                source = cls._safe_encode_and_convert(tokenizer, text)
                logging.debug(f"æ–‡æœ¬{i}: Tokenizationç»“æœ: {source[:10]}... (å…±{len(source)}ä¸ª)")
                sources.append(source)
            
            # ä¿®å¤target_prefixæ ¼å¼ - æ ¹æ®å®˜æ–¹demoï¼Œæ¯ä¸ªæ–‡æœ¬éƒ½éœ€è¦å¯¹åº”çš„ç›®æ ‡è¯­è¨€å‰ç¼€
            # ctranslate2æœŸæœ›: List[Optional[List[str]]] æ ¼å¼
            mapped_tgt_lang = cls._map_language_code(tgt_lang)  # æ˜ å°„è¯­è¨€ä»£ç 
            target_prefix = [[mapped_tgt_lang] for _ in texts]  # æ¢å¤ï¼šæ‰¹é‡ç¿»è¯‘éœ€è¦è¿™ä¸ªæ ¼å¼
            
            # æ‰§è¡Œæ‰¹é‡ç¿»è¯‘ - ä¿®å¤APIè°ƒç”¨æ ¼å¼
            # ctranslate2æœŸæœ›: List[List[str]] æ ¼å¼çš„tokenizedæ–‡æœ¬
            results = translator.translate_batch(sources, target_prefix=target_prefix)
            
            # ä¿®å¤decodeè°ƒç”¨ - æ ¹æ®å®˜æ–¹demoå¤„ç†ç»“æœ
            translated_texts = []
            for hyp in results:
                try:
                    if hasattr(hyp, 'hypotheses') and hyp.hypotheses:
                        # æ ¹æ®å®˜æ–¹demoï¼šè·³è¿‡ç¬¬ä¸€ä¸ªtoken
                        target = hyp.hypotheses[0][1:]
                        
                        # ä½¿ç”¨convert_tokens_to_ids + decode
                        if hasattr(tokenizer, 'convert_tokens_to_ids') and hasattr(tokenizer, 'decode'):
                            target_ids = tokenizer.convert_tokens_to_ids(target)
                            translated_text = tokenizer.decode(target_ids)
                        else:
                            # å›é€€ï¼šç›´æ¥ä½¿ç”¨hypotheses
                            translated_text = ' '.join(target)
                    else:
                        # å›é€€å¤„ç†
                        translated_text = str(hyp)
                    
                    translated_texts.append(translated_text)
                    
                except Exception as e:
                    logging.error(f"å¤„ç†ç¿»è¯‘ç»“æœå¤±è´¥: {e}")
                    # å›é€€åˆ°åŸå§‹ç»“æœ
                    translated_texts.append(str(hyp))
            
            # è®°å½•å“åº”æ—¶é—´
            response_time = time.time() - start_time
            if "response_times" in model_instance:
                model_instance["response_times"].append(response_time)
                # åªä¿ç•™æœ€è¿‘çš„10ä¸ªå“åº”æ—¶é—´
                if len(model_instance["response_times"]) > 10:
                    model_instance["response_times"] = model_instance["response_times"][-10:]
            
            return translated_texts
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            logging.error(f"æ‰¹é‡ç¿»è¯‘æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise HTTPException(status_code=500, detail=f"æ‰¹é‡ç¿»è¯‘æœåŠ¡å†…éƒ¨é”™è¯¯: {e}")
        finally:
            if model_instance:
                cls._release_model(model_instance)

    @classmethod
    def get_gpu_status(cls):
        """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
        try:
            # æ›´æ–°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            cls._update_gpu_memory_usage(cls._cuda_instances)
            
            status = {
                "gpu_count": len(cls._gpu_devices),
                "gpu_devices": cls._gpu_devices,
                "gpu_instances": len(cls._cuda_instances),
                "cpu_instances": len(cls._cpu_instances),
                "memory_info": cls._gpu_memory_info,
                "instance_details": [],
                "performance_metrics": {},
                "task_type_distribution": cls._task_type_distribution,
                "text_instances": cls._text_translation_instances,
                "file_instances": cls._file_translation_instances,
                "timestamp": time.time()
            }
            
            # æ·»åŠ å®ä¾‹è¯¦ç»†ä¿¡æ¯
            total_tasks = 0
            total_successful = 0
            total_text_tasks = 0
            total_file_tasks = 0
            avg_response_time = 0
            response_times = []
            
            for instance in cls._cuda_instances:
                instance_info = {
                    "id": instance["id"],
                    "gpu_id": instance.get("gpu_id", "N/A"),
                    "type": instance["type"],
                    "task_type": instance.get("task_type", "unknown"),
                    "available_slots": instance["task_count"]._value,
                    "response_times": instance.get("response_times", []),
                    "memory_usage": instance.get("memory_usage", 0),
                    "total_tasks": instance.get("total_tasks", 0),
                    "successful_tasks": instance.get("successful_tasks", 0),
                    "text_tasks": instance.get("text_tasks", 0),
                    "file_tasks": instance.get("file_tasks", 0),
                    "success_rate": 0
                }
                
                # è®¡ç®—æˆåŠŸç‡
                if instance_info["total_tasks"] > 0:
                    instance_info["success_rate"] = instance_info["successful_tasks"] / instance_info["total_tasks"]
                
                # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
                if instance_info["response_times"]:
                    instance_avg_response = sum(instance_info["response_times"]) / len(instance_info["response_times"])
                    instance_info["avg_response_time"] = instance_avg_response
                    response_times.extend(instance_info["response_times"])
                
                status["instance_details"].append(instance_info)
                
                # ç´¯è®¡ç»Ÿè®¡
                total_tasks += instance_info["total_tasks"]
                total_successful += instance_info["successful_tasks"]
                total_text_tasks += instance_info["text_tasks"]
                total_file_tasks += instance_info["file_tasks"]
            
            # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡
            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
            
            status["performance_metrics"] = {
                "total_tasks": total_tasks,
                "total_successful": total_successful,
                "total_text_tasks": total_text_tasks,
                "total_file_tasks": total_file_tasks,
                "overall_success_rate": total_successful / total_tasks if total_tasks > 0 else 0,
                "avg_response_time": avg_response_time,
                "total_instances": len(cls._cuda_instances) + len(cls._cpu_instances),
                "active_instances": sum(1 for inst in cls._cuda_instances if inst["task_count"]._value < config_manager.getint('GPU', 'GPU_CONCURRENT_LIMIT', 4)),
                "task_type_balance": {
                    "text_tasks_percentage": (total_text_tasks / total_tasks * 100) if total_tasks > 0 else 0,
                    "file_tasks_percentage": (total_file_tasks / total_tasks * 100) if total_tasks > 0 else 0
                }
            }
            
            return status
        except Exception as e:
            logging.error(f"è·å–GPUçŠ¶æ€å¤±è´¥: {e}")
            return {"error": str(e)}

    @classmethod
    def _map_language_code(cls, lang_code: str) -> str:
        """æ˜ å°„è¯­è¨€ä»£ç åˆ°NLLBæ¨¡å‹æœŸæœ›çš„æ ¼å¼"""
        language_mapping = {
            # å‰ç«¯è¯­è¨€ä»£ç  -> NLLBå®˜æ–¹è¯­è¨€ä»£ç 
            "zh_Hans": "zho_Hans",  # ä¸­æ–‡ç®€ä½“ -> NLLBå®˜æ–¹ä»£ç 
            "zh_Hant": "zho_Hant",  # ä¸­æ–‡ç¹ä½“ -> NLLBå®˜æ–¹ä»£ç 
            "en": "eng_Latn",       # è‹±è¯­
            "eng": "eng_Latn",      # è‹±è¯­
            "eng_Latn": "eng_Latn", # è‹±è¯­ï¼ˆå‰ç«¯æ ¼å¼ï¼‰
            "mon": "khk_Cyrl",      # è’™å¤è¯­ -> NLLBå®˜æ–¹ä»£ç 
            "mn": "khk_Cyrl",       # è’™å¤è¯­ -> NLLBå®˜æ–¹ä»£ç 
            "khk_Cyrl": "khk_Cyrl", # è¥¿é‡Œå°”è’™æ–‡ï¼ˆå‰ç«¯æ ¼å¼ï¼‰-> NLLBå®˜æ–¹ä»£ç 
            "khk": "khk_Cyrl",      # è’™å¤è¯­ç®€å†™ -> NLLBå®˜æ–¹ä»£ç 
            # å¦‚æœå·²ç»æ˜¯NLLBå®˜æ–¹æ ¼å¼ï¼Œç›´æ¥è¿”å›
            "zho_Hans": "zho_Hans", # NLLBä¸­æ–‡ç®€ä½“
            "zho_Hant": "zho_Hant", # NLLBä¸­æ–‡ç¹ä½“
            "eng_Latn": "eng_Latn", # NLLBè‹±è¯­
            "khk_Cyrl": "khk_Cyrl", # NLLBè’™å¤è¯­
        }
        return language_mapping.get(lang_code, lang_code)

    @classmethod
    def _simple_tokenize(cls, text: str) -> list:
        """ç®€å•çš„æ–‡æœ¬åˆ†å‰²ï¼Œç»•è¿‡tokenizeré—®é¢˜"""
        try:
            # å°è¯•ä½¿ç”¨transformers tokenizer
            if 'transformers' in sys.modules:
                try:
                    from transformers import AutoTokenizer
                    # ä½¿ç”¨é»˜è®¤çš„tokenizer
                    tokenizer = AutoTokenizer.from_pretrained("facebook/nllb-200-distilled-600M")
                    tokens = tokenizer.tokenize(text)
                    if tokens:
                        return tokens
                except Exception as e:
                    logging.warning(f"Transformers tokenizerå¤±è´¥: {e}")
            
            # å›é€€åˆ°ç®€å•åˆ†å‰²
            return text.split()
            
        except Exception as e:
            logging.warning(f"ç®€å•tokenizeå¤±è´¥: {e}")
            return text.split()
    
    @classmethod
    def _safe_encode_and_convert(cls, tokenizer, text: str) -> list:
        """å®‰å…¨çš„ç¼–ç å’Œè½¬æ¢ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ"""
        try:
            # æ–¹æ³•1ï¼šå°è¯•æ ‡å‡†æµç¨‹
            try:
                raw_tokens = tokenizer.encode(text)
                logging.debug(f"æ ‡å‡†encodeæˆåŠŸ: ç±»å‹={type(raw_tokens)}, å€¼={raw_tokens}")
                
                # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                if isinstance(raw_tokens, (list, tuple)):
                    token_ids = []
                    for item in raw_tokens:
                        try:
                            token_ids.append(int(item))
                        except (ValueError, TypeError):
                            logging.warning(f"è·³è¿‡æ— æ•ˆtoken: {item}")
                    
                    if token_ids:
                        tokens = tokenizer.convert_ids_to_tokens(token_ids)
                        if tokens:
                            return tokens
                
            except Exception as e:
                logging.warning(f"æ ‡å‡†æµç¨‹å¤±è´¥: {e}")
            
            # æ–¹æ³•2ï¼šå°è¯•ç›´æ¥tokenize
            try:
                if hasattr(tokenizer, 'tokenize'):
                    tokens = tokenizer.tokenize(text)
                    if tokens:
                        return tokens
            except Exception as e:
                logging.warning(f"ç›´æ¥tokenizeå¤±è´¥: {e}")
            
            # æ–¹æ³•3ï¼šä½¿ç”¨ç®€å•åˆ†å‰²
            logging.info("ä½¿ç”¨ç®€å•æ–‡æœ¬åˆ†å‰²ä½œä¸ºå›é€€")
            return text.split()
            
        except Exception as e:
            logging.error(f"æ‰€æœ‰tokenizationæ–¹æ³•éƒ½å¤±è´¥: {e}")
            return text.split()


class DocxTranslator(TranslatorSingleton):
    @staticmethod
    def translate_run(run, src_lang, tgt_lang, via_eng=False):
        if not run.text.strip():
            return ""

        translated_lines = TranslatorSingleton.translate_batch(texts=run.text.split("\n"),
                                                              src_lang=src_lang,
                                                              tgt_lang=tgt_lang,
                                                              use_cuda=True,
                                                              via_eng=via_eng,
                                                              task_type="file")  # ä½¿ç”¨æ–‡ä»¶ç¿»è¯‘å®ä¾‹
        return '\n'.join(translated_lines)

    @staticmethod
    def translate_paragraph(paragraph, src_lang, tgt_lang, via_eng=False):
        translated_runs = []

        for run in paragraph.runs:
            translated_text = DocxTranslator.translate_run(run=run,
                                                           src_lang=src_lang,
                                                           tgt_lang=tgt_lang,
                                                           via_eng=via_eng)
            translated_runs.append((translated_text, run))

        paragraph.clear()

        for translated_text, original_run in translated_runs:
            translated_run = paragraph.add_run(translated_text)

            translated_run.bold = original_run.bold
            translated_run.italic = original_run.italic
            translated_run.underline = original_run.underline
            translated_run.font.size = original_run.font.size
            translated_run.font.name = original_run.font.name
            translated_run.font.color.rgb = original_run.font.color.rgb
            translated_run.font.highlight_color = original_run.font.highlight_color

    @staticmethod
    def translate_docx(input_path: str, output_path: str, src_lang: str, tgt_lang: str, via_eng=False):
        doc = Document(input_path)
        translated_doc = Document()

        for para in tqdm(doc.paragraphs, desc=f"Translating {input_path}"):
            if para.text.strip():
                DocxTranslator.translate_paragraph(paragraph=para,
                                                   src_lang=src_lang,
                                                   tgt_lang=tgt_lang,
                                                   via_eng=via_eng)
                translated_doc.add_paragraph(para.text)

        translated_doc.save(output_path)


class TableTranslator(TranslatorSingleton):
    @staticmethod
    def translate_text(text, src_lang, tgt_lang, via_eng=False):
        if text is None:
            return text
        lines = text.split('\n')
        translated_lines = TranslatorSingleton.translate_batch(texts=lines,
                                                               src_lang=src_lang,
                                                               tgt_lang=tgt_lang,
                                                               use_cuda=True,
                                                               via_eng=via_eng,
                                                               task_type="file")  # ä½¿ç”¨æ–‡ä»¶ç¿»è¯‘å®ä¾‹
        return '\n'.join(translated_lines)

    @staticmethod
    def translate_excel(input_path: str, output_path: str, src_lang: str, tgt_lang: str, via_eng=False):
        """
        ä¼˜åŒ–åçš„Excelç¿»è¯‘åŠŸèƒ½
        æ”¯æŒæ‰€æœ‰å·¥ä½œè¡¨çš„ç¿»è¯‘ï¼Œæ­£ç¡®å¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œåªè¯»å•å…ƒæ ¼
        """
        try:
            print(f"å¼€å§‹ç¿»è¯‘Excelæ–‡ä»¶: {input_path}")
            wb = load_workbook(input_path)
            
            # è·å–å·¥ä½œè¡¨ä¿¡æ¯
            total_sheets = len(wb.worksheets)
            print(f"Excelæ–‡ä»¶åŒ…å« {total_sheets} ä¸ªå·¥ä½œè¡¨")
            
            # ç»Ÿè®¡æ‰€æœ‰å·¥ä½œè¡¨çš„ç¿»è¯‘ä¿¡æ¯
            total_cells_translated = 0
            total_texts_translated = 0
            
            for sheet_idx, sheet in enumerate(wb.worksheets, 1):
                print(f"\n{'='*50}")
                print(f"æ­£åœ¨ç¿»è¯‘å·¥ä½œè¡¨ {sheet_idx}/{total_sheets}: {sheet.title}")
                print(f"å·¥ä½œè¡¨å°ºå¯¸: {sheet.max_row} è¡Œ x {sheet.max_column} åˆ—")
                
                # å­˜å‚¨éœ€è¦ç¿»è¯‘çš„å•å…ƒæ ¼å†…å®¹
                cells_to_translate = []
                
                # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬å†…å®¹
                print(f"æ­£åœ¨æ‰«æå·¥ä½œè¡¨ {sheet.title} çš„å•å…ƒæ ¼...")
                for row in sheet.iter_rows():
                    for cell in row:
                        try:
                            # å®‰å…¨åœ°è·å–å•å…ƒæ ¼å€¼
                            cell_value = cell.value
                            
                            # åªç¿»è¯‘éç©ºå­—ç¬¦ä¸²
                            if cell_value and isinstance(cell_value, str) and cell_value.strip():
                                # æ£€æŸ¥æ˜¯å¦ä¸ºåˆå¹¶å•å…ƒæ ¼çš„ä¸»å•å…ƒæ ¼
                                is_merge_master = False
                                for merged_range in sheet.merged_cells.ranges:
                                    if cell.coordinate == merged_range.start_cell.coordinate:
                                        is_merge_master = True
                                        break
                                
                                # åªç¿»è¯‘åˆå¹¶å•å…ƒæ ¼çš„ä¸»å•å…ƒæ ¼æˆ–éåˆå¹¶å•å…ƒæ ¼
                                if is_merge_master or not any(cell.coordinate in merged_range for merged_range in sheet.merged_cells.ranges):
                                    cells_to_translate.append({
                                        'coordinate': cell.coordinate,
                                        'value': cell_value,
                                        'is_merged': is_merge_master
                                    })
                        except Exception as e:
                            print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–å•å…ƒæ ¼ {cell.coordinate}: {e}")
                            continue
                
                print(f"å·¥ä½œè¡¨ {sheet.title} æ‰¾åˆ° {len(cells_to_translate)} ä¸ªéœ€è¦ç¿»è¯‘çš„å•å…ƒæ ¼")
                total_cells_translated += len(cells_to_translate)
                
                # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ç¿»è¯‘æ–‡æœ¬
                if cells_to_translate:
                    # æå–æ‰€æœ‰æ–‡æœ¬è¿›è¡Œæ‰¹é‡ç¿»è¯‘
                    texts_to_translate = [cell['value'] for cell in cells_to_translate]
                    print(f"å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(texts_to_translate)} ä¸ªæ–‡æœ¬...")
                    
                    try:
                        translated_texts = TranslatorSingleton.translate_batch(
                            texts=texts_to_translate,
                            src_lang=src_lang,
                            tgt_lang=tgt_lang,
                            use_cuda=True,
                            via_eng=via_eng,
                            task_type="file"
                        )
                        
                        # ç¬¬ä¸‰æ­¥ï¼šå°†ç¿»è¯‘ç»“æœåº”ç”¨åˆ°å·¥ä½œè¡¨
                        print(f"æ­£åœ¨å°†ç¿»è¯‘ç»“æœåº”ç”¨åˆ°å·¥ä½œè¡¨ {sheet.title}...")
                        for i, cell_info in enumerate(cells_to_translate):
                            try:
                                coord = cell_info['coordinate']
                                translated_text = translated_texts[i] if i < len(translated_texts) else cell_info['value']
                                
                                # è®¾ç½®å•å…ƒæ ¼å€¼
                                sheet[coord].value = translated_text
                                
                                # å¦‚æœæ˜¯åˆå¹¶å•å…ƒæ ¼ï¼Œéœ€è¦åº”ç”¨åˆ°æ•´ä¸ªåˆå¹¶èŒƒå›´
                                if cell_info['is_merged']:
                                    for merged_range in sheet.merged_cells.ranges:
                                        if coord == merged_range.start_cell.coordinate:
                                            # åˆå¹¶å•å…ƒæ ¼çš„å€¼ä¼šè‡ªåŠ¨åº”ç”¨åˆ°æ•´ä¸ªèŒƒå›´
                                            print(f"  âœ“ å·²ç¿»è¯‘åˆå¹¶å•å…ƒæ ¼ {coord}: {cell_info['value']} -> {translated_text}")
                                            break
                                else:
                                    print(f"  âœ“ å·²ç¿»è¯‘å•å…ƒæ ¼ {coord}: {cell_info['value']} -> {translated_text}")
                                    
                                total_texts_translated += 1
                                    
                            except Exception as e:
                                print(f"è­¦å‘Šï¼šæ— æ³•è®¾ç½®å•å…ƒæ ¼ {coord} çš„å€¼: {e}")
                                continue
                    
                    except Exception as e:
                        print(f"å·¥ä½œè¡¨ {sheet.title} ç¿»è¯‘å¤±è´¥: {e}")
                        # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œä¿æŒåŸå€¼
                        continue
                else:
                    print(f"å·¥ä½œè¡¨ {sheet.title} æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
                
                # ç¬¬å››æ­¥ï¼šä¿å­˜å·¥ä½œè¡¨
                print(f"âœ“ å·¥ä½œè¡¨ {sheet.title} ç¿»è¯‘å®Œæˆ")
            
            # ä¿å­˜å·¥ä½œç°¿
            print(f"\n{'='*50}")
            print(f"æ‰€æœ‰å·¥ä½œè¡¨ç¿»è¯‘å®Œæˆï¼Œæ­£åœ¨ä¿å­˜æ–‡ä»¶...")
            wb.save(output_path)
            print(f"âœ“ Excelæ–‡ä»¶ç¿»è¯‘å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_path}")
            print(f"ğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
            print(f"  - æ€»å·¥ä½œè¡¨æ•°: {total_sheets}")
            print(f"  - æ€»å•å…ƒæ ¼æ•°: {total_cells_translated}")
            print(f"  - æ€»ç¿»è¯‘æ–‡æœ¬: {total_texts_translated}")
            
        except Exception as e:
            print(f"Excelç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise e

    @staticmethod
    def translate_csv(input_path: str, output_path: str, src_lang: str, tgt_lang: str, via_eng=False):
        """
        ä¼˜åŒ–åçš„CSVç¿»è¯‘åŠŸèƒ½
        æ”¯æŒå¤§æ–‡ä»¶å¤„ç†å’Œé”™è¯¯æ¢å¤
        """
        try:
            print(f"å¼€å§‹ç¿»è¯‘CSVæ–‡ä»¶: {input_path}")
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(input_path)
            print(f"CSVæ–‡ä»¶åŒ…å« {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
            print(f"åˆ—å: {list(df.columns)}")
            
            # æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬
            texts_to_translate = []
            text_positions = []  # è®°å½•æ–‡æœ¬åœ¨DataFrameä¸­çš„ä½ç½®
            
            print("æ­£åœ¨æ‰«æCSVæ–‡ä»¶ä¸­çš„æ–‡æœ¬å†…å®¹...")
            for col_idx, column in enumerate(df.columns):
                print(f"  æ‰«æåˆ— {col_idx+1}/{len(df.columns)}: {column}")
                for row_idx, value in enumerate(df[column]):
                    if value is not None and isinstance(value, str) and value.strip():
                        texts_to_translate.append(value)
                        text_positions.append((row_idx, col_idx))
            
            print(f"æ‰¾åˆ° {len(texts_to_translate)} ä¸ªéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
            
            if texts_to_translate:
                try:
                    print(f"å¼€å§‹æ‰¹é‡ç¿»è¯‘ {len(texts_to_translate)} ä¸ªæ–‡æœ¬...")
                    # æ‰¹é‡ç¿»è¯‘
                    translated_texts = TranslatorSingleton.translate_batch(
                        texts=texts_to_translate,
                        src_lang=src_lang,
                        tgt_lang=tgt_lang,
                        use_cuda=True,
                        via_eng=via_eng,
                        task_type="file"
                    )
                    
                    print(f"ç¿»è¯‘å®Œæˆï¼Œæ­£åœ¨å°†ç»“æœåº”ç”¨åˆ°CSVæ–‡ä»¶...")
                    # å°†ç¿»è¯‘ç»“æœåº”ç”¨åˆ°DataFrame
                    for i, (row_idx, col_idx) in enumerate(text_positions):
                        if i < len(translated_texts):
                            df.iloc[row_idx, col_idx] = translated_texts[i]
                            print(f"  âœ“ å·²ç¿»è¯‘: {texts_to_translate[i]} -> {translated_texts[i]}")
                        else:
                            print(f"è­¦å‘Šï¼šç¿»è¯‘ç»“æœä¸è¶³ï¼Œä¿æŒåŸå€¼: {texts_to_translate[i]}")
                    
                except Exception as e:
                    print(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                    # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œä¿æŒåŸå€¼
                    pass
            else:
                print("CSVæ–‡ä»¶ä¸­æ²¡æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
            
            # ä¿å­˜ç¿»è¯‘åçš„CSVæ–‡ä»¶
            print("æ­£åœ¨ä¿å­˜ç¿»è¯‘åçš„CSVæ–‡ä»¶...")
            df.to_csv(output_path, index=False)
            print(f"âœ“ CSVæ–‡ä»¶ç¿»è¯‘å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {output_path}")
            print(f"ğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
            print(f"  - æ€»è¡Œæ•°: {len(df)}")
            print(f"  - æ€»åˆ—æ•°: {len(df.columns)}")
            print(f"  - ç¿»è¯‘æ–‡æœ¬æ•°: {len(texts_to_translate)}")
            
        except Exception as e:
            print(f"CSVç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            raise e
