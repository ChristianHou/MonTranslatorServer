#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NLLB-MoE 54B æ¨¡å‹ä¸‹è½½è„šæœ¬
ä»Hugging Faceä¸‹è½½facebook/nllb-moe-54bæ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•

ä½¿ç”¨æ–¹æ³•ï¼š
python download_nllb_moe.py

éœ€è¦å®‰è£…ä¾èµ–ï¼š
pip install huggingface_hub tqdm requests
"""

import os
import sys
from pathlib import Path
from huggingface_hub import snapshot_download, HfApi
from tqdm import tqdm
import requests
import time
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_nllb_moe.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class NLLBMoEDownloader:
    def __init__(self, cache_dir="./cache"):
        self.model_id = "facebook/nllb-moe-54b"
        self.cache_dir = Path(cache_dir).absolute()
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©huggingfaceä½¿ç”¨æŒ‡å®šçš„ç¼“å­˜ç›®å½•
        os.environ['HF_HOME'] = str(self.cache_dir)
        os.environ['HUGGINGFACE_HUB_CACHE'] = str(self.cache_dir / "huggingface")

        logger.info(f"ç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"æ¨¡å‹ID: {self.model_id}")

    def check_model_info(self):
        """æ£€æŸ¥æ¨¡å‹ä¿¡æ¯"""
        logger.info("=== æ£€æŸ¥æ¨¡å‹ä¿¡æ¯ ===")

        try:
            api = HfApi()
            model_info = api.model_info(self.model_id)

            logger.info(f"æ¨¡å‹åç§°: {model_info.modelId}")
            logger.info(f"æ ‡ç­¾: {model_info.tags}")
            logger.info(f"ä¸‹è½½æ¬¡æ•°: {model_info.downloads}")

            # è·å–æ–‡ä»¶åˆ—è¡¨
            files = api.list_repo_files(self.model_id)
            logger.info(f"æ–‡ä»¶æ•°é‡: {len(files)}")

            # è®¡ç®—æ€»å¤§å°
            total_size = 0
            for file in files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                logger.info(f"  - {file}")

            return True

        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
            return False

    def download_model(self, resume=True):
        """ä¸‹è½½æ¨¡å‹"""
        logger.info("=== å¼€å§‹ä¸‹è½½æ¨¡å‹ ===")
        logger.info(f"ç›®æ ‡ç›®å½•: {self.cache_dir}")

        try:
            # é…ç½®ä¸‹è½½å‚æ•°
            download_kwargs = {
                "repo_id": self.model_id,
                "local_dir": self.cache_dir / "models--facebook--nllb-moe-54b",
                "local_dir_use_symlinks": False,  # ä½¿ç”¨ç¡¬é“¾æ¥è€Œä¸æ˜¯è½¯é“¾æ¥
                "resume_download": resume,
                "max_workers": 4,  # åŒæ—¶ä¸‹è½½çš„çº¿ç¨‹æ•°
                "tqdm_class": tqdm,
            }

            logger.info("å¼€å§‹ä¸‹è½½...")
            start_time = time.time()

            # æ‰§è¡Œä¸‹è½½
            snapshot_path = snapshot_download(**download_kwargs)

            end_time = time.time()
            duration = end_time - start_time

            logger.info("âœ… ä¸‹è½½å®Œæˆ!"            logger.info(".2f"            logger.info(f"æ¨¡å‹ä¿å­˜ä½ç½®: {snapshot_path}")

            return snapshot_path

        except KeyboardInterrupt:
            logger.info("ä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­")
            return None
        except Exception as e:
            logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
            logger.info("å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•")
            return None

    def verify_download(self, model_path):
        """éªŒè¯ä¸‹è½½çš„å®Œæ•´æ€§"""
        logger.info("=== éªŒè¯ä¸‹è½½ ===")

        if not model_path or not Path(model_path).exists():
            logger.error("æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
            return False

        try:
            # æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                "config.json",
                "tokenizer.json",
                "tokenizer_config.json",
                "pytorch_model.bin.index.json"
            ]

            missing_files = []
            for file in required_files:
                if not (Path(model_path) / file).exists():
                    missing_files.append(file)

            if missing_files:
                logger.warning(f"ç¼ºå°‘æ–‡ä»¶: {missing_files}")
                return False

            # è®¡ç®—ç›®å½•å¤§å°
            total_size = 0
            file_count = 0
            for file_path in Path(model_path).rglob("*"):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
                    file_count += 1

            logger.info(","            logger.info(".2f"
            return True

        except Exception as e:
            logger.error(f"éªŒè¯å¤±è´¥: {e}")
            return False

    def cleanup_cache(self):
        """æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è½½"""
        logger.info("=== æ¸…ç†ç¼“å­˜ ===")

        try:
            import shutil

            # æŸ¥æ‰¾ä¸å®Œæ•´çš„ä¸‹è½½
            incomplete_dirs = []
            for item in self.cache_dir.iterdir():
                if item.is_dir() and item.name.startswith("models--facebook--nllb-moe-54b"):
                    # æ£€æŸ¥æ˜¯å¦æœ‰.lockæ–‡ä»¶
                    lock_file = item / ".huggingface" / "download" / ".lock"
                    if lock_file.exists():
                        incomplete_dirs.append(item)

            if incomplete_dirs:
                logger.info(f"å‘ç° {len(incomplete_dirs)} ä¸ªä¸å®Œæ•´çš„ä¸‹è½½")
                for incomplete_dir in incomplete_dirs:
                    logger.info(f"åˆ é™¤: {incomplete_dir}")
                    shutil.rmtree(incomplete_dir, ignore_errors=True)
                logger.info("âœ… æ¸…ç†å®Œæˆ")
            else:
                logger.info("æ²¡æœ‰å‘ç°ä¸å®Œæ•´çš„ä¸‹è½½")

        except Exception as e:
            logger.error(f"æ¸…ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("NLLB-MoE 54B æ¨¡å‹ä¸‹è½½å™¨")
    print("=" * 60)
    print(f"æ¨¡å‹: facebook/nllb-moe-54b")
    print(f"å¤§å°: ~200GB (è¿™æ˜¯ä¸€ä¸ªéå¸¸å¤§çš„æ¨¡å‹)")
    print("=" * 60)

    # æ£€æŸ¥å‚æ•°
    if len(sys.argv) > 1:
        cache_dir = sys.argv[1]
    else:
        cache_dir = "./cache"

    # åˆ›å»ºä¸‹è½½å™¨
    downloader = NLLBMoEDownloader(cache_dir)

    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    try:
        stat = os.statvfs(cache_dir)
        free_space = stat.f_bavail * stat.f_frsize / (1024**3)  # GB
        logger.info(".2f"
        if free_space < 250:
            logger.warning("ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³ï¼Œå»ºè®®è‡³å°‘ä¿ç•™250GBç©ºé—´")
            response = input("æ˜¯å¦ç»§ç»­? (y/N): ").lower().strip()
            if response not in ['y', 'yes']:
                logger.info("ä¸‹è½½å·²å–æ¶ˆ")
                return
    except Exception as e:
        logger.warning(f"æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}")

    # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
    if not downloader.check_model_info():
        logger.error("æ— æ³•è·å–æ¨¡å‹ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return

    # æ¸…ç†ä¸å®Œæ•´çš„ä¸‹è½½
    downloader.cleanup_cache()

    # å¼€å§‹ä¸‹è½½
    print("\nâš ï¸  è­¦å‘Š:")
    print("- è¿™ä¸ªæ¨¡å‹éå¸¸å¤§ï¼ˆçº¦200GBï¼‰")
    print("- ä¸‹è½½å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´")
    print("- è¯·ç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®š")
    print("- å¯ä»¥éšæ—¶æŒ‰Ctrl+Cä¸­æ–­å¹¶åœ¨ä¸‹æ¬¡ç»§ç»­")

    response = input("\næ˜¯å¦å¼€å§‹ä¸‹è½½? (y/N): ").lower().strip()
    if response not in ['y', 'yes']:
        logger.info("ä¸‹è½½å·²å–æ¶ˆ")
        return

    # æ‰§è¡Œä¸‹è½½
    model_path = downloader.download_model()

    if model_path:
        # éªŒè¯ä¸‹è½½
        if downloader.verify_download(model_path):
            logger.info("ğŸ‰ ä¸‹è½½å’ŒéªŒè¯éƒ½æˆåŠŸå®Œæˆ!")
            logger.info(f"æ¨¡å‹ä½ç½®: {model_path}")
            logger.info("ç°åœ¨å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œç¿»è¯‘äº†")
        else:
            logger.warning("ä¸‹è½½å®Œæˆä½†éªŒè¯å¤±è´¥ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
    else:
        logger.error("ä¸‹è½½å¤±è´¥")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nä¸‹è½½è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)

