#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
å¯¹æ¯”ctranslate2å’Œtransformersåœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šçš„æ€§èƒ½å·®å¼‚
"""

import time
import logging
import sys
import os
from typing import List, Dict, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_ctranslate2_performance(model_path: str, test_texts: List[str], batch_size: int = 8) -> Dict[str, Any]:
    """æµ‹è¯•ctranslate2æ€§èƒ½"""
    try:
        import ctranslate2
        
        logger.info("ğŸ§ª æµ‹è¯•ctranslate2æ€§èƒ½...")
        
        # åŠ è½½æ¨¡å‹
        start_time = time.time()
        translator = ctranslate2.Translator(
            model_path,
            device="cpu",  # ä½¿ç”¨CPUè¿›è¡Œå…¬å¹³å¯¹æ¯”
            inter_threads=4,
            intra_threads=1
        )
        load_time = time.time() - start_time
        logger.info(f"âœ… ctranslate2æ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
        
        # æµ‹è¯•æ¨ç†æ€§èƒ½
        start_time = time.time()
        
        # æ‰¹å¤„ç†ç¿»è¯‘
        if batch_size > 1:
            # åˆ†æ‰¹å¤„ç†
            all_results = []
            for i in range(0, len(test_texts), batch_size):
                batch = test_texts[i:i + batch_size]
                results = translator.translate_batch(batch, target_prefix=["cmn_Hans"])
                all_results.extend(results)
        else:
            # å•å¥å¤„ç†
            all_results = []
            for text in test_texts:
                result = translator.translate(text, target_prefix="cmn_Hans")
                all_results.append(result)
        
        inference_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_chars = sum(len(text) for text in test_texts)
        chars_per_second = total_chars / inference_time if inference_time > 0 else 0
        
        logger.info(f"âœ… ctranslate2æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        logger.info(f"âœ… å¤„ç†å­—ç¬¦æ•°: {total_chars}")
        logger.info(f"âœ… å­—ç¬¦/ç§’: {chars_per_second:.0f}")
        
        return {
            "success": True,
            "load_time": load_time,
            "inference_time": inference_time,
            "total_chars": total_chars,
            "chars_per_second": chars_per_second,
            "results": all_results
        }
        
    except Exception as e:
        logger.error(f"âŒ ctranslate2æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def test_transformers_performance(model_name: str, test_texts: List[str], batch_size: int = 8) -> Dict[str, Any]:
    """æµ‹è¯•transformersæ€§èƒ½"""
    try:
        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
        import torch
        
        logger.info("ğŸ§ª æµ‹è¯•transformersæ€§èƒ½...")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        load_time = time.time() - start_time
        logger.info(f"âœ… transformersæ¨¡å‹åŠ è½½æ—¶é—´: {load_time:.2f}ç§’")
        
        # æµ‹è¯•æ¨ç†æ€§èƒ½
        start_time = time.time()
        
        # æ‰¹å¤„ç†ç¿»è¯‘
        if batch_size > 1:
            # åˆ†æ‰¹å¤„ç†
            all_results = []
            for i in range(0, len(test_texts), batch_size):
                batch = test_texts[i:i + batch_size]
                
                # ç¼–ç 
                inputs = tokenizer(batch, return_tensors="pt", padding=True, truncation=True, max_length=512)
                
                # æ¨ç†
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=512,
                        num_beams=5,
                        early_stopping=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                # è§£ç 
                results = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
                all_results.extend(results)
        else:
            # å•å¥å¤„ç†
            all_results = []
            for text in test_texts:
                inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
                
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=512,
                        num_beams=5,
                        early_stopping=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                all_results.append(result)
        
        inference_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_chars = sum(len(text) for text in test_texts)
        chars_per_second = total_chars / inference_time if inference_time > 0 else 0
        
        logger.info(f"âœ… transformersæ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        logger.info(f"âœ… å¤„ç†å­—ç¬¦æ•°: {total_chars}")
        logger.info(f"âœ… å­—ç¬¦/ç§’: {chars_per_second:.0f}")
        
        return {
            "success": True,
            "load_time": load_time,
            "inference_time": inference_time,
            "total_chars": total_chars,
            "chars_per_second": chars_per_second,
            "results": all_results
        }
        
    except Exception as e:
        logger.error(f"âŒ transformersæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}


def generate_test_texts() -> List[str]:
    """ç”Ÿæˆæµ‹è¯•æ–‡æœ¬"""
    test_texts = [
        "Hello, how are you today?",
        "The weather is beautiful today.",
        "Machine learning is an exciting field.",
        "Natural language processing helps computers understand human language.",
        "Translation is the process of converting text from one language to another.",
        "Artificial intelligence is transforming many industries.",
        "Deep learning models can achieve remarkable performance on various tasks.",
        "The development of neural networks has revolutionized computer science.",
        "Computational linguistics combines linguistics and computer science.",
        "Modern translation systems use sophisticated algorithms and large datasets."
    ]
    return test_texts


def compare_performance(ctranslate2_result: Dict[str, Any], transformers_result: Dict[str, Any]) -> None:
    """å¯¹æ¯”æ€§èƒ½ç»“æœ"""
    logger.info(f"\n{'='*60}")
    logger.info("æ€§èƒ½å¯¹æ¯”ç»“æœ")
    logger.info(f"{'='*60}")
    
    if not ctranslate2_result["success"] or not transformers_result["success"]:
        logger.error("âŒ æ— æ³•å®Œæˆæ€§èƒ½å¯¹æ¯”ï¼Œå› ä¸ºæŸä¸ªæµ‹è¯•å¤±è´¥")
        return
    
    # åŠ è½½æ—¶é—´å¯¹æ¯”
    c2_load = ctranslate2_result["load_time"]
    tf_load = transformers_result["load_time"]
    load_ratio = tf_load / c2_load if c2_load > 0 else float('inf')
    
    logger.info(f"ğŸ“Š æ¨¡å‹åŠ è½½æ—¶é—´å¯¹æ¯”:")
    logger.info(f"  - ctranslate2: {c2_load:.2f}ç§’")
    logger.info(f"  - transformers: {tf_load:.2f}ç§’")
    logger.info(f"  - æ¯”ä¾‹: transformers/ctranslate2 = {load_ratio:.2f}x")
    
    # æ¨ç†æ—¶é—´å¯¹æ¯”
    c2_inference = ctranslate2_result["inference_time"]
    tf_inference = transformers_result["inference_time"]
    inference_ratio = tf_inference / c2_inference if c2_inference > 0 else float('inf')
    
    logger.info(f"\nğŸ“Š æ¨ç†æ€§èƒ½å¯¹æ¯”:")
    logger.info(f"  - ctranslate2: {c2_inference:.2f}ç§’")
    logger.info(f"  - transformers: {tf_inference:.2f}ç§’")
    logger.info(f"  - æ¯”ä¾‹: transformers/ctranslate2 = {inference_ratio:.2f}x")
    
    # å­—ç¬¦å¤„ç†é€Ÿåº¦å¯¹æ¯”
    c2_speed = ctranslate2_result["chars_per_second"]
    tf_speed = transformers_result["chars_per_second"]
    speed_ratio = c2_speed / tf_speed if tf_speed > 0 else float('inf')
    
    logger.info(f"\nğŸ“Š å¤„ç†é€Ÿåº¦å¯¹æ¯”:")
    logger.info(f"  - ctranslate2: {c2_speed:.0f} å­—ç¬¦/ç§’")
    logger.info(f"  - transformers: {tf_speed:.0f} å­—ç¬¦/ç§’")
    logger.info(f"  - æ¯”ä¾‹: ctranslate2/transformers = {speed_ratio:.2f}x")
    
    # æ€»ç»“
    logger.info(f"\nğŸ¯ æ€§èƒ½æ€»ç»“:")
    if inference_ratio > 2.0:
        logger.info(f"âœ… ctranslate2 æ˜¾è‘—æ›´å¿« ({inference_ratio:.1f}x)")
    elif inference_ratio > 1.5:
        logger.info(f"âœ… ctranslate2 æ˜æ˜¾æ›´å¿« ({inference_ratio:.1f}x)")
    elif inference_ratio > 1.2:
        logger.info(f"âœ… ctranslate2 ç¨å¿« ({inference_ratio:.1f}x)")
    else:
        logger.info(f"â„¹ï¸  æ€§èƒ½å·®å¼‚ä¸å¤§ ({inference_ratio:.1f}x)")
    
    # å»ºè®®
    logger.info(f"\nğŸ’¡ å»ºè®®:")
    if inference_ratio > 2.0:
        logger.info("  - å¼ºçƒˆæ¨èä½¿ç”¨ ctranslate2ï¼Œæ€§èƒ½æå‡æ˜¾è‘—")
    elif inference_ratio > 1.5:
        logger.info("  - æ¨èä½¿ç”¨ ctranslate2ï¼Œæ€§èƒ½æå‡æ˜æ˜¾")
    elif inference_ratio > 1.2:
        logger.info("  - å¯ä»¥è€ƒè™‘ä½¿ç”¨ ctranslate2ï¼Œæ€§èƒ½ç•¥æœ‰æå‡")
    else:
        logger.info("  - æ€§èƒ½å·®å¼‚ä¸å¤§ï¼Œå¯ä»¥æ ¹æ®æ˜“ç”¨æ€§é€‰æ‹©")
        logger.info("  - å¦‚æœ ctranslate2 é…ç½®å¤æ‚ï¼Œä½¿ç”¨ transformers ä¹Ÿæ˜¯ä¸é”™çš„é€‰æ‹©")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•...")
    
    # ç”Ÿæˆæµ‹è¯•æ–‡æœ¬
    test_texts = generate_test_texts()
    logger.info(f"ğŸ“ æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    
    # è·å–æ¨¡å‹è·¯å¾„
    try:
        from utils.config_manager import config_manager
        model_name = config_manager.get("SETTINGS", "SEQ_TRANSLATE_MODEL")
        model_path = config_manager.get_model_path(model_name)
        
        logger.info(f"ğŸ” ä½¿ç”¨æ¨¡å‹: {model_name}")
        logger.info(f"ğŸ” æ¨¡å‹è·¯å¾„: {model_path}")
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡å‹é…ç½®å¤±è´¥: {e}")
        logger.info("ğŸ’¡ è¯·ç¡®ä¿é…ç½®æ–‡ä»¶æ­£ç¡®ï¼Œæˆ–è€…æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„")
        return 1
    
    # æµ‹è¯•ctranslate2æ€§èƒ½
    logger.info(f"\n{'='*50}")
    c2_result = test_ctranslate2_performance(model_path, test_texts, batch_size=8)
    
    # æµ‹è¯•transformersæ€§èƒ½
    logger.info(f"\n{'='*50}")
    tf_result = test_transformers_performance(model_name, test_texts, batch_size=8)
    
    # å¯¹æ¯”æ€§èƒ½
    if c2_result["success"] and tf_result["success"]:
        compare_performance(c2_result, tf_result)
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        with open("performance_comparison_results.txt", "w", encoding="utf-8") as f:
            f.write("æ€§èƒ½å¯¹æ¯”æµ‹è¯•ç»“æœ\n")
            f.write("=" * 50 + "\n")
            f.write(f"ctranslate2 æ¨ç†æ—¶é—´: {c2_result['inference_time']:.2f}ç§’\n")
            f.write(f"transformers æ¨ç†æ—¶é—´: {tf_result['inference_time']:.2f}ç§’\n")
            f.write(f"æ€§èƒ½æ¯”ä¾‹: transformers/ctranslate2 = {tf_result['inference_time']/c2_result['inference_time']:.2f}x\n")
        
        logger.info(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: performance_comparison_results.txt")
        
        return 0
    else:
        logger.error("âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•å®Œæˆå¯¹æ¯”")
        return 1


if __name__ == "__main__":
    exit(main())
